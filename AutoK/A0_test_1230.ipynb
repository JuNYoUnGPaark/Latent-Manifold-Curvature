{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oYSo88e1lt8",
        "outputId": "3c90c99c-81d3-487a-f5c2-3baf5f06e502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 1.39 | [Pred(win): 19.6 / GT: 21]\n",
            "[Fold TEST Summary] subject1 | GT=21 | Pred(win)=19.61 | Diff=-1.39 | k_hat(full)=1.02 | phase_entropy(full)=0.038\n",
            "Fold  2 | Test: subject2 | MAE: 6.36 | [Pred(win): 25.4 / GT: 19]\n",
            "[Fold TEST Summary] subject2 | GT=19 | Pred(win)=25.36 | Diff=+6.36 | k_hat(full)=1.06 | phase_entropy(full)=0.119\n",
            "Fold  3 | Test: subject3 | MAE: 6.60 | [Pred(win): 27.6 / GT: 21]\n",
            "[Fold TEST Summary] subject3 | GT=21 | Pred(win)=27.60 | Diff=+6.60 | k_hat(full)=1.01 | phase_entropy(full)=0.023\n",
            "Fold  4 | Test: subject4 | MAE: 2.46 | [Pred(win): 22.5 / GT: 20]\n",
            "[Fold TEST Summary] subject4 | GT=20 | Pred(win)=22.46 | Diff=+2.46 | k_hat(full)=1.02 | phase_entropy(full)=0.045\n",
            "Fold  5 | Test: subject5 | MAE: 5.74 | [Pred(win): 25.7 / GT: 20]\n",
            "[Fold TEST Summary] subject5 | GT=20 | Pred(win)=25.74 | Diff=+5.74 | k_hat(full)=1.01 | phase_entropy(full)=0.035\n",
            "Fold  6 | Test: subject6 | MAE: 6.26 | [Pred(win): 13.7 / GT: 20]\n",
            "[Fold TEST Summary] subject6 | GT=20 | Pred(win)=13.74 | Diff=-6.26 | k_hat(full)=1.01 | phase_entropy(full)=0.018\n",
            "Fold  7 | Test: subject7 | MAE: 5.78 | [Pred(win): 25.8 / GT: 20]\n",
            "[Fold TEST Summary] subject7 | GT=20 | Pred(win)=25.78 | Diff=+5.78 | k_hat(full)=1.01 | phase_entropy(full)=0.037\n",
            "Fold  8 | Test: subject8 | MAE: 5.57 | [Pred(win): 15.4 / GT: 21]\n",
            "[Fold TEST Summary] subject8 | GT=21 | Pred(win)=15.43 | Diff=-5.57 | k_hat(full)=1.01 | phase_entropy(full)=0.018\n",
            "Fold  9 | Test: subject9 | MAE: 11.76 | [Pred(win): 32.8 / GT: 21]\n",
            "[Fold TEST Summary] subject9 | GT=21 | Pred(win)=32.76 | Diff=+11.76 | k_hat(full)=1.01 | phase_entropy(full)=0.017\n",
            "Fold 10 | Test: subject10 | MAE: 0.71 | [Pred(win): 19.3 / GT: 20]\n",
            "[Fold TEST Summary] subject10 | GT=20 | Pred(win)=19.29 | Diff=-0.71 | k_hat(full)=1.02 | phase_entropy(full)=0.042\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 5.261\n",
            " >>> Standard Deviation: 3.006\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ]\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 6, 21),\n",
        "            (\"subject2\", 6, 19),\n",
        "            (\"subject3\", 6, 21),\n",
        "            (\"subject4\", 6, 20),\n",
        "            (\"subject5\", 6, 20),\n",
        "            (\"subject6\", 6, 20),\n",
        "            (\"subject7\", 6, 20),\n",
        "            (\"subject8\", 6, 21),\n",
        "            (\"subject9\", 6, 21),\n",
        "            (\"subject10\", 6, 20),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ]\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 7, 20),\n",
        "            (\"subject2\", 7, 20),\n",
        "            (\"subject3\", 7, 20),\n",
        "            (\"subject4\", 7, 20),\n",
        "            (\"subject5\", 7, 20),\n",
        "            (\"subject6\", 7, 20),\n",
        "            (\"subject7\", 7, 20),\n",
        "            (\"subject8\", 7, 19),\n",
        "            (\"subject9\", 7, 19),\n",
        "            (\"subject10\", 7, 20),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN5b7AtrVn4l",
        "outputId": "e1883bb9-14e6-4f02-a0c7-39d943933bdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 0.09 | [Pred(win): 19.9 / GT: 20]\n",
            "[Fold TEST Summary] subject1 | GT=20 | Pred(win)=19.91 | Diff=-0.09 | k_hat(full)=1.01 | phase_entropy(full)=0.028\n",
            "Fold  2 | Test: subject2 | MAE: 2.63 | [Pred(win): 22.6 / GT: 20]\n",
            "[Fold TEST Summary] subject2 | GT=20 | Pred(win)=22.63 | Diff=+2.63 | k_hat(full)=1.01 | phase_entropy(full)=0.020\n",
            "Fold  3 | Test: subject3 | MAE: 4.18 | [Pred(win): 24.2 / GT: 20]\n",
            "[Fold TEST Summary] subject3 | GT=20 | Pred(win)=24.18 | Diff=+4.18 | k_hat(full)=1.01 | phase_entropy(full)=0.026\n",
            "Fold  4 | Test: subject4 | MAE: 1.62 | [Pred(win): 21.6 / GT: 20]\n",
            "[Fold TEST Summary] subject4 | GT=20 | Pred(win)=21.62 | Diff=+1.62 | k_hat(full)=1.01 | phase_entropy(full)=0.026\n",
            "Fold  5 | Test: subject5 | MAE: 1.50 | [Pred(win): 18.5 / GT: 20]\n",
            "[Fold TEST Summary] subject5 | GT=20 | Pred(win)=18.50 | Diff=-1.50 | k_hat(full)=1.01 | phase_entropy(full)=0.027\n",
            "Fold  6 | Test: subject6 | MAE: 7.18 | [Pred(win): 12.8 / GT: 20]\n",
            "[Fold TEST Summary] subject6 | GT=20 | Pred(win)=12.82 | Diff=-7.18 | k_hat(full)=1.01 | phase_entropy(full)=0.016\n",
            "Fold  7 | Test: subject7 | MAE: 1.63 | [Pred(win): 18.4 / GT: 20]\n",
            "[Fold TEST Summary] subject7 | GT=20 | Pred(win)=18.37 | Diff=-1.63 | k_hat(full)=1.01 | phase_entropy(full)=0.019\n",
            "Fold  8 | Test: subject8 | MAE: 1.98 | [Pred(win): 21.0 / GT: 19]\n",
            "[Fold TEST Summary] subject8 | GT=19 | Pred(win)=20.98 | Diff=+1.98 | k_hat(full)=1.01 | phase_entropy(full)=0.022\n",
            "Fold  9 | Test: subject9 | MAE: 1.43 | [Pred(win): 20.4 / GT: 19]\n",
            "[Fold TEST Summary] subject9 | GT=19 | Pred(win)=20.43 | Diff=+1.43 | k_hat(full)=1.01 | phase_entropy(full)=0.028\n",
            "Fold 10 | Test: subject10 | MAE: 1.00 | [Pred(win): 19.0 / GT: 20]\n",
            "[Fold TEST Summary] subject10 | GT=20 | Pred(win)=19.00 | Diff=-1.00 | k_hat(full)=1.01 | phase_entropy(full)=0.032\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 2.324\n",
            " >>> Standard Deviation: 1.910\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ]\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 8, 20),\n",
        "            (\"subject2\", 8, 21),\n",
        "            (\"subject3\", 8, 21),\n",
        "            (\"subject4\", 8, 19),\n",
        "            (\"subject5\", 8, 20),\n",
        "            (\"subject6\", 8, 20),\n",
        "            (\"subject7\", 8, 21),\n",
        "            (\"subject8\", 8, 21),\n",
        "            (\"subject9\", 8, 21),\n",
        "            (\"subject10\", 8, 21),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5wa5kHyWD5j",
        "outputId": "6d850081-3b6f-42d2-b452-ddb4026be7c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 12.18 | [Pred(win): 32.2 / GT: 20]\n",
            "[Fold TEST Summary] subject1 | GT=20 | Pred(win)=32.18 | Diff=+12.18 | k_hat(full)=1.04 | phase_entropy(full)=0.084\n",
            "Fold  2 | Test: subject2 | MAE: 1.54 | [Pred(win): 22.5 / GT: 21]\n",
            "[Fold TEST Summary] subject2 | GT=21 | Pred(win)=22.54 | Diff=+1.54 | k_hat(full)=1.05 | phase_entropy(full)=0.103\n",
            "Fold  3 | Test: subject3 | MAE: 4.99 | [Pred(win): 16.0 / GT: 21]\n",
            "[Fold TEST Summary] subject3 | GT=21 | Pred(win)=16.01 | Diff=-4.99 | k_hat(full)=1.04 | phase_entropy(full)=0.085\n",
            "Fold  4 | Test: subject4 | MAE: 5.68 | [Pred(win): 24.7 / GT: 19]\n",
            "[Fold TEST Summary] subject4 | GT=19 | Pred(win)=24.68 | Diff=+5.68 | k_hat(full)=1.79 | phase_entropy(full)=0.248\n",
            "Fold  5 | Test: subject5 | MAE: 1.30 | [Pred(win): 18.7 / GT: 20]\n",
            "[Fold TEST Summary] subject5 | GT=20 | Pred(win)=18.70 | Diff=-1.30 | k_hat(full)=1.04 | phase_entropy(full)=0.076\n",
            "Fold  6 | Test: subject6 | MAE: 4.94 | [Pred(win): 15.1 / GT: 20]\n",
            "[Fold TEST Summary] subject6 | GT=20 | Pred(win)=15.06 | Diff=-4.94 | k_hat(full)=1.01 | phase_entropy(full)=0.037\n",
            "Fold  7 | Test: subject7 | MAE: 2.24 | [Pred(win): 18.8 / GT: 21]\n",
            "[Fold TEST Summary] subject7 | GT=21 | Pred(win)=18.76 | Diff=-2.24 | k_hat(full)=1.02 | phase_entropy(full)=0.049\n",
            "Fold  8 | Test: subject8 | MAE: 3.25 | [Pred(win): 17.7 / GT: 21]\n",
            "[Fold TEST Summary] subject8 | GT=21 | Pred(win)=17.75 | Diff=-3.25 | k_hat(full)=1.05 | phase_entropy(full)=0.101\n",
            "Fold  9 | Test: subject9 | MAE: 4.32 | [Pred(win): 25.3 / GT: 21]\n",
            "[Fold TEST Summary] subject9 | GT=21 | Pred(win)=25.32 | Diff=+4.32 | k_hat(full)=1.01 | phase_entropy(full)=0.032\n",
            "Fold 10 | Test: subject10 | MAE: 2.31 | [Pred(win): 18.7 / GT: 21]\n",
            "[Fold TEST Summary] subject10 | GT=21 | Pred(win)=18.69 | Diff=-2.31 | k_hat(full)=1.02 | phase_entropy(full)=0.059\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 4.274\n",
            " >>> Standard Deviation: 3.012\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ]\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 12, 20),\n",
        "            (\"subject2\", 12, 22),\n",
        "            (\"subject3\", 12, 21),\n",
        "            (\"subject4\", 12, 21),\n",
        "            (\"subject5\", 12, 20),\n",
        "            (\"subject6\", 12, 21),\n",
        "            (\"subject7\", 12, 19),\n",
        "            (\"subject8\", 12, 20),\n",
        "            (\"subject9\", 12, 20),\n",
        "            (\"subject10\", 12, 20),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1sHdIkRWQZd",
        "outputId": "14be26ad-f153-4c7c-8381-e74c1045e993"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 0.42 | [Pred(win): 20.4 / GT: 20]\n",
            "[Fold TEST Summary] subject1 | GT=20 | Pred(win)=20.42 | Diff=+0.42 | k_hat(full)=2.68 | phase_entropy(full)=0.880\n",
            "Fold  2 | Test: subject2 | MAE: 0.30 | [Pred(win): 22.3 / GT: 22]\n",
            "[Fold TEST Summary] subject2 | GT=22 | Pred(win)=22.30 | Diff=+0.30 | k_hat(full)=1.73 | phase_entropy(full)=0.663\n",
            "Fold  3 | Test: subject3 | MAE: 0.33 | [Pred(win): 20.7 / GT: 21]\n",
            "[Fold TEST Summary] subject3 | GT=21 | Pred(win)=20.67 | Diff=-0.33 | k_hat(full)=2.24 | phase_entropy(full)=0.783\n",
            "Fold  4 | Test: subject4 | MAE: 2.14 | [Pred(win): 23.1 / GT: 21]\n",
            "[Fold TEST Summary] subject4 | GT=21 | Pred(win)=23.14 | Diff=+2.14 | k_hat(full)=2.25 | phase_entropy(full)=0.838\n",
            "Fold  5 | Test: subject5 | MAE: 3.56 | [Pred(win): 16.4 / GT: 20]\n",
            "[Fold TEST Summary] subject5 | GT=20 | Pred(win)=16.44 | Diff=-3.56 | k_hat(full)=2.59 | phase_entropy(full)=0.898\n",
            "Fold  6 | Test: subject6 | MAE: 0.31 | [Pred(win): 21.3 / GT: 21]\n",
            "[Fold TEST Summary] subject6 | GT=21 | Pred(win)=21.31 | Diff=+0.31 | k_hat(full)=2.68 | phase_entropy(full)=0.884\n",
            "Fold  7 | Test: subject7 | MAE: 0.61 | [Pred(win): 18.4 / GT: 19]\n",
            "[Fold TEST Summary] subject7 | GT=19 | Pred(win)=18.39 | Diff=-0.61 | k_hat(full)=2.74 | phase_entropy(full)=0.896\n",
            "Fold  8 | Test: subject8 | MAE: 0.03 | [Pred(win): 20.0 / GT: 20]\n",
            "[Fold TEST Summary] subject8 | GT=20 | Pred(win)=19.97 | Diff=-0.03 | k_hat(full)=2.17 | phase_entropy(full)=0.851\n",
            "Fold  9 | Test: subject9 | MAE: 1.28 | [Pred(win): 18.7 / GT: 20]\n",
            "[Fold TEST Summary] subject9 | GT=20 | Pred(win)=18.72 | Diff=-1.28 | k_hat(full)=2.77 | phase_entropy(full)=0.872\n",
            "Fold 10 | Test: subject10 | MAE: 2.05 | [Pred(win): 22.1 / GT: 20]\n",
            "[Fold TEST Summary] subject10 | GT=20 | Pred(win)=22.05 | Diff=+2.05 | k_hat(full)=2.24 | phase_entropy(full)=0.788\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 1.103\n",
            " >>> Standard Deviation: 1.083\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            4: 'Walking',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            4: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 4, 113),\n",
        "            (\"subject2\", 4, 99),\n",
        "            (\"subject3\", 4, 104),\n",
        "            (\"subject4\", 4, 112),\n",
        "            (\"subject5\", 4, 109),\n",
        "            (\"subject6\", 4, 111),\n",
        "            (\"subject7\", 4, 106),\n",
        "            (\"subject8\", 4, 95),\n",
        "            (\"subject9\", 4, 111),\n",
        "            (\"subject10\", 4, 102),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzl2ZvZagzu0",
        "outputId": "0afdc8e6-381a-4aba-f3c1-41b0907ceef3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 15.00 | [Pred(win): 98.0 / GT: 113]\n",
            "[Fold TEST Summary] subject1 | GT=113 | Pred(win)=98.00 | Diff=-15.00 | k_hat(full)=2.36 | phase_entropy(full)=0.627\n",
            "Fold  2 | Test: subject2 | MAE: 10.34 | [Pred(win): 109.3 / GT: 99]\n",
            "[Fold TEST Summary] subject2 | GT=99 | Pred(win)=109.34 | Diff=+10.34 | k_hat(full)=2.36 | phase_entropy(full)=0.660\n",
            "Fold  3 | Test: subject3 | MAE: 6.00 | [Pred(win): 110.0 / GT: 104]\n",
            "[Fold TEST Summary] subject3 | GT=104 | Pred(win)=110.00 | Diff=+6.00 | k_hat(full)=2.21 | phase_entropy(full)=0.633\n",
            "Fold  4 | Test: subject4 | MAE: 1.85 | [Pred(win): 110.1 / GT: 112]\n",
            "[Fold TEST Summary] subject4 | GT=112 | Pred(win)=110.15 | Diff=-1.85 | k_hat(full)=2.19 | phase_entropy(full)=0.601\n",
            "Fold  5 | Test: subject5 | MAE: 18.03 | [Pred(win): 91.0 / GT: 109]\n",
            "[Fold TEST Summary] subject5 | GT=109 | Pred(win)=90.97 | Diff=-18.03 | k_hat(full)=2.56 | phase_entropy(full)=0.731\n",
            "Fold  6 | Test: subject6 | MAE: 14.76 | [Pred(win): 96.2 / GT: 111]\n",
            "[Fold TEST Summary] subject6 | GT=111 | Pred(win)=96.24 | Diff=-14.76 | k_hat(full)=2.23 | phase_entropy(full)=0.759\n",
            "Fold  7 | Test: subject7 | MAE: 2.78 | [Pred(win): 103.2 / GT: 106]\n",
            "[Fold TEST Summary] subject7 | GT=106 | Pred(win)=103.22 | Diff=-2.78 | k_hat(full)=2.31 | phase_entropy(full)=0.753\n",
            "Fold  8 | Test: subject8 | MAE: 4.99 | [Pred(win): 100.0 / GT: 95]\n",
            "[Fold TEST Summary] subject8 | GT=95 | Pred(win)=99.99 | Diff=+4.99 | k_hat(full)=2.34 | phase_entropy(full)=0.674\n",
            "Fold  9 | Test: subject9 | MAE: 7.54 | [Pred(win): 118.5 / GT: 111]\n",
            "[Fold TEST Summary] subject9 | GT=111 | Pred(win)=118.54 | Diff=+7.54 | k_hat(full)=2.00 | phase_entropy(full)=0.569\n",
            "Fold 10 | Test: subject10 | MAE: 4.65 | [Pred(win): 106.7 / GT: 102]\n",
            "[Fold TEST Summary] subject10 | GT=102 | Pred(win)=106.65 | Diff=+4.65 | k_hat(full)=2.08 | phase_entropy(full)=0.620\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 8.594\n",
            " >>> Standard Deviation: 5.354\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            4: 'Walking',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            4: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 10, 157),\n",
        "            (\"subject2\", 10, 161),\n",
        "            (\"subject3\", 10, 154),\n",
        "            (\"subject4\", 10, 154),\n",
        "            (\"subject5\", 10, 160),\n",
        "            (\"subject6\", 10, 156),\n",
        "            (\"subject7\", 10, 153),\n",
        "            (\"subject8\", 10, 160),\n",
        "            (\"subject9\", 10, 166),\n",
        "            (\"subject10\", 10, 156),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s94q8I4bhW-B",
        "outputId": "a485fc06-ba1f-49f2-d217-885e3a1f0bc4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 3.03 | [Pred(win): 154.0 / GT: 157]\n",
            "[Fold TEST Summary] subject1 | GT=157 | Pred(win)=153.97 | Diff=-3.03 | k_hat(full)=1.29 | phase_entropy(full)=0.365\n",
            "Fold  2 | Test: subject2 | MAE: 5.05 | [Pred(win): 166.0 / GT: 161]\n",
            "[Fold TEST Summary] subject2 | GT=161 | Pred(win)=166.05 | Diff=+5.05 | k_hat(full)=1.74 | phase_entropy(full)=0.504\n",
            "Fold  3 | Test: subject3 | MAE: 6.57 | [Pred(win): 147.4 / GT: 154]\n",
            "[Fold TEST Summary] subject3 | GT=154 | Pred(win)=147.43 | Diff=-6.57 | k_hat(full)=1.47 | phase_entropy(full)=0.345\n",
            "Fold  4 | Test: subject4 | MAE: 4.35 | [Pred(win): 158.3 / GT: 154]\n",
            "[Fold TEST Summary] subject4 | GT=154 | Pred(win)=158.35 | Diff=+4.35 | k_hat(full)=1.44 | phase_entropy(full)=0.309\n",
            "Fold  5 | Test: subject5 | MAE: 2.54 | [Pred(win): 157.5 / GT: 160]\n",
            "[Fold TEST Summary] subject5 | GT=160 | Pred(win)=157.46 | Diff=-2.54 | k_hat(full)=1.53 | phase_entropy(full)=0.320\n",
            "Fold  6 | Test: subject6 | MAE: 41.59 | [Pred(win): 197.6 / GT: 156]\n",
            "[Fold TEST Summary] subject6 | GT=156 | Pred(win)=197.59 | Diff=+41.59 | k_hat(full)=1.37 | phase_entropy(full)=0.410\n",
            "Fold  7 | Test: subject7 | MAE: 11.78 | [Pred(win): 141.2 / GT: 153]\n",
            "[Fold TEST Summary] subject7 | GT=153 | Pred(win)=141.22 | Diff=-11.78 | k_hat(full)=1.66 | phase_entropy(full)=0.388\n",
            "Fold  8 | Test: subject8 | MAE: 13.48 | [Pred(win): 146.5 / GT: 160]\n",
            "[Fold TEST Summary] subject8 | GT=160 | Pred(win)=146.52 | Diff=-13.48 | k_hat(full)=1.49 | phase_entropy(full)=0.420\n",
            "Fold  9 | Test: subject9 | MAE: 8.15 | [Pred(win): 174.2 / GT: 166]\n",
            "[Fold TEST Summary] subject9 | GT=166 | Pred(win)=174.15 | Diff=+8.15 | k_hat(full)=1.67 | phase_entropy(full)=0.313\n",
            "Fold 10 | Test: subject10 | MAE: 7.68 | [Pred(win): 163.7 / GT: 156]\n",
            "[Fold TEST Summary] subject10 | GT=156 | Pred(win)=163.68 | Diff=+7.68 | k_hat(full)=1.54 | phase_entropy(full)=0.353\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 10.421\n",
            " >>> Standard Deviation: 10.920\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) + Windowing version\n",
        "#\n",
        "# ✅ Windowing added:\n",
        "# - TRAIN: trial -> sliding windows (window-level count = trial-average rate * window duration)\n",
        "# - TEST : trial 그대로 두고, windowing inference로 window rate 평균 -> 전체 count 예측\n",
        "# - k_hat / entropy / rep_rate / phase heatmap은 (표현학습 확인용) full-trial 1회 forward로 기록\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,              # (T, C)\n",
        "                'count': float(gt_count),      # trial total count\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) ✅ Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=8.0, stride_sec=4.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    TRAIN 전용: trial -> sliding windows 확장\n",
        "    window 라벨은 trial-level 평균 rate로부터 생성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]  # (T,C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    TEST 전용: trial -> sliding windows inference -> window rate 평균 -> total count\n",
        "    x_np: (T,C) numpy (이미 정규화된 상태)\n",
        "    return: pred_count(float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    # short trial -> 1회 forward\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.8) Dataset / Collate\n",
        "# ---------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = micro_rate(t) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        # ✅ BUGFIX: use tau argument (was tau=1.0 fixed)\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p  # (B,T,K)\n",
        "\n",
        "        micro_rate_t = amp_t  # (B,T)\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6) # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,\n",
        "            \"phase_p\": phase_p,\n",
        "            \"phase_logits\": phase_logits,\n",
        "            \"micro_rate_t\": micro_rate_t,\n",
        "            \"rep_rate_t\": rep_rate_t,\n",
        "            \"k_hat\": k_hat,\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred(win) {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat(full)={k_hat:.2f} | phase_entropy(full)={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (LOSO)\n",
        "# ---------------------------------------------------------------------\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            4: 'Walking',\n",
        "            10: 'Jogging',\n",
        "            11: 'Running',\n",
        "\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            4: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            10: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "            11: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "                'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'\n",
        "                ],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # ✅ Windowing Params (added)\n",
        "        \"win_sec\": 8.0,\n",
        "        \"stride_sec\": 4.0,\n",
        "        \"drop_last\": True,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0075,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # Count-only labels\n",
        "        \"ALL_LABELS\": [\n",
        "            (\"subject1\", 11, 165),\n",
        "            (\"subject2\", 11, 158),\n",
        "            (\"subject3\", 11, 174),\n",
        "            (\"subject4\", 11, 163),\n",
        "            (\"subject5\", 11, 157),\n",
        "            (\"subject6\", 11, 172),\n",
        "            (\"subject7\", 11, 149),\n",
        "            (\"subject8\", 11, 166),\n",
        "            (\"subject9\", 11, 174),\n",
        "            (\"subject10\", 11, 172),\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "    loso_results = []\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\" >>> Starting LOSO (count-only, K-auto) + WINDOWING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    viz_cache = []\n",
        "\n",
        "    for fold_idx, test_subj in enumerate(subjects):\n",
        "        set_strict_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n",
        "        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n",
        "\n",
        "        # ---- trial-level ----\n",
        "        train_trials = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_trials  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if not test_trials:\n",
        "            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n",
        "            continue\n",
        "\n",
        "        # ✅ TRAIN only: trial -> windows\n",
        "        train_data = trial_list_to_windows(\n",
        "            train_trials,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"win_sec\"],\n",
        "            stride_sec=CONFIG[\"stride_sec\"],\n",
        "            drop_last=CONFIG[\"drop_last\"]\n",
        "        )\n",
        "\n",
        "        # TEST: trial 그대로\n",
        "        test_data = test_trials\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            generator=g,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # ---- fold test (trial 단위) ----\n",
        "        fold_mae = 0.0\n",
        "        fold_res_str = \"\"\n",
        "\n",
        "        test_gt = None\n",
        "        test_pred = None\n",
        "        test_diff = None\n",
        "        test_khat = None\n",
        "        test_entropy = None\n",
        "\n",
        "        test_t = None\n",
        "        test_rep_rate = None\n",
        "        test_phase_p = None\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]  # (T,C)\n",
        "            T = x_np.shape[0]\n",
        "\n",
        "            # ✅ pred: windowing inference\n",
        "            count_pred_win, _win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"win_sec\"],\n",
        "                stride_sec=CONFIG[\"stride_sec\"],\n",
        "                device=device,\n",
        "                tau=CONFIG.get(\"tau\", 1.0),\n",
        "                batch_size=CONFIG.get(\"batch_size\", 64)\n",
        "            )\n",
        "\n",
        "            count_gt = float(item[\"count\"])\n",
        "            abs_err = abs(count_pred_win - count_gt)\n",
        "            fold_mae += abs_err\n",
        "            fold_res_str += f\"[Pred(win): {count_pred_win:.1f} / GT: {count_gt:.0f}]\"\n",
        "\n",
        "            # ✅ 표현학습 확인용: full-trial 1회 forward (k_hat/entropy/rep_rate/phase)\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "            with torch.no_grad():\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG.get(\"tau\", 1.0))\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "\n",
        "            test_gt = count_gt\n",
        "            test_pred = float(count_pred_win)\n",
        "            test_diff = float(count_pred_win - count_gt)\n",
        "            test_khat = k_hat\n",
        "            test_entropy = ent\n",
        "\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n",
        "            test_rep_rate = rep_rate\n",
        "            test_t = np.arange(T) / CONFIG[\"fs\"]\n",
        "            test_phase_p = phase_p\n",
        "\n",
        "        fold_mae /= len(test_data)\n",
        "        loso_results.append(fold_mae)\n",
        "\n",
        "        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n",
        "\n",
        "        if (test_gt is not None) and (test_pred is not None):\n",
        "            print(\n",
        "                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred(win)={test_pred:.2f} | \"\n",
        "                f\"Diff={test_diff:+.2f} | k_hat(full)={test_khat:.2f} | phase_entropy(full)={test_entropy:.3f}\"\n",
        "            )\n",
        "\n",
        "        if (test_t is not None) and (test_rep_rate is not None):\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_idx + 1,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"t\": test_t,\n",
        "                \"rep_rate\": test_rep_rate,\n",
        "                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n",
        "                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n",
        "                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n",
        "                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n",
        "                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n",
        "                \"phase_p\": test_phase_p,\n",
        "            })\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n",
        "    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=\"Fold-wise TEST visualization (Pred by window-avg rate)\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat(full)={item['k_hat']:.2f} | ent(full)={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GATi3HFthm-K",
        "outputId": "08050ccd-989d-4197-efc9-6c4c7cb46713"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Starting LOSO (count-only, K-auto) + WINDOWING\n",
            "--------------------------------------------------------------------------------\n",
            "Fold  1 | Test: subject1 | MAE: 2.92 | [Pred(win): 162.1 / GT: 165]\n",
            "[Fold TEST Summary] subject1 | GT=165 | Pred(win)=162.08 | Diff=-2.92 | k_hat(full)=1.62 | phase_entropy(full)=0.315\n",
            "Fold  2 | Test: subject2 | MAE: 14.48 | [Pred(win): 172.5 / GT: 158]\n",
            "[Fold TEST Summary] subject2 | GT=158 | Pred(win)=172.48 | Diff=+14.48 | k_hat(full)=1.98 | phase_entropy(full)=0.317\n",
            "Fold  3 | Test: subject3 | MAE: 21.68 | [Pred(win): 152.3 / GT: 174]\n",
            "[Fold TEST Summary] subject3 | GT=174 | Pred(win)=152.32 | Diff=-21.68 | k_hat(full)=1.35 | phase_entropy(full)=0.309\n",
            "Fold  4 | Test: subject4 | MAE: 19.88 | [Pred(win): 182.9 / GT: 163]\n",
            "[Fold TEST Summary] subject4 | GT=163 | Pred(win)=182.88 | Diff=+19.88 | k_hat(full)=1.29 | phase_entropy(full)=0.283\n",
            "Fold  5 | Test: subject5 | MAE: 1.47 | [Pred(win): 158.5 / GT: 157]\n",
            "[Fold TEST Summary] subject5 | GT=157 | Pred(win)=158.47 | Diff=+1.47 | k_hat(full)=1.50 | phase_entropy(full)=0.309\n",
            "Fold  6 | Test: subject6 | MAE: 24.01 | [Pred(win): 148.0 / GT: 172]\n",
            "[Fold TEST Summary] subject6 | GT=172 | Pred(win)=147.99 | Diff=-24.01 | k_hat(full)=1.96 | phase_entropy(full)=0.395\n",
            "Fold  7 | Test: subject7 | MAE: 16.87 | [Pred(win): 165.9 / GT: 149]\n",
            "[Fold TEST Summary] subject7 | GT=149 | Pred(win)=165.87 | Diff=+16.87 | k_hat(full)=1.36 | phase_entropy(full)=0.259\n",
            "Fold  8 | Test: subject8 | MAE: 11.74 | [Pred(win): 154.3 / GT: 166]\n",
            "[Fold TEST Summary] subject8 | GT=166 | Pred(win)=154.26 | Diff=-11.74 | k_hat(full)=1.48 | phase_entropy(full)=0.373\n",
            "Fold  9 | Test: subject9 | MAE: 17.30 | [Pred(win): 191.3 / GT: 174]\n",
            "[Fold TEST Summary] subject9 | GT=174 | Pred(win)=191.30 | Diff=+17.30 | k_hat(full)=1.57 | phase_entropy(full)=0.294\n",
            "Fold 10 | Test: subject10 | MAE: 21.76 | [Pred(win): 150.2 / GT: 172]\n",
            "[Fold TEST Summary] subject10 | GT=172 | Pred(win)=150.24 | Diff=-21.76 | k_hat(full)=1.49 | phase_entropy(full)=0.334\n",
            "--------------------------------------------------------------------------------\n",
            " >>> Final LOSO Result (Average MAE): 15.211\n",
            " >>> Standard Deviation: 7.371\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}