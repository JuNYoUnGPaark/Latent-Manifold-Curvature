{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from scipy.signal import find_peaks\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Strict Seeding\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_strict_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"[Info] Seed fixed to {seed}\")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Data Loading\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            # Get raw data (N, C)\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Normalize (Z-score per trial)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Model Classes\n",
        "# ------------------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=3):\n",
        "        super().__init__()\n",
        "        # Global pooling to capture window context if used primarily for reconstruction\n",
        "        # But for sequence processing, we might want to keep temporal dimension\n",
        "        # Here we use a architecture that preserves T somewhat or encodes window\n",
        "        # For simplicity in this 'Trial-based' approach, let's keep it simple:\n",
        "        # Encoder: (B, C, T) -> (B, D, T) mapping strictly time-wise\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1) # Project to latent\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x) # (B, D, T)\n",
        "        z = z.transpose(1, 2) # (B, T, D) for easier handling later\n",
        "        return z\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        # z: (B, T, D) -> (B, D, T) 로 바꾼 뒤 Conv1d로 복원\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)  # 최종 채널을 입력 채널 C로\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)       # (B, D, T)\n",
        "        x_hat = self.net(zt)         # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "class RateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=3, hidden=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        return self.net(z).squeeze(-1)  # (B, T)\n",
        "\n",
        "class RateModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = RateHead(latent_dim, hidden_dim)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "        # softplus로 양수 rate\n",
        "        nn.init.constant_(self.rate_head.net[-1].bias, -2.0)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x: (B, C, T)\n",
        "        z = self.encoder(x)  # (B, T, D)\n",
        "        x_hat = self.decoder(z)  # (B, C, T)\n",
        "\n",
        "        rate_logits = self.rate_head(z)\n",
        "        instant_rate = F.softplus(rate_logits)\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rate = instant_rate.mean(dim=1)  # (B, )\n",
        "        else:\n",
        "            mask = mask.to(dtype=instant_rate.dtype, device=instant_rate.device)\n",
        "            avg_rate = (instant_rate * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        return avg_rate, z, x_hat\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Dataset & Collate\n",
        "# ------------------------------------------------------------------------------\n",
        "class TrialDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps the loaded data to provide (sequence, count, meta)\n",
        "    Compatible with Variable Length Collate\n",
        "    \"\"\"\n",
        "    def __init__(self, trial_list):\n",
        "        # trial_list: list of dicts {'data': np.array(T, C), 'count': float, 'meta': str}\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1) # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    # batch: list of (data, count, meta)\n",
        "    # data: (C, T)\n",
        "\n",
        "    # 1. Find max length\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        # Pad data\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data), # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),       # (B, T_max)\n",
        "        \"count\": torch.stack(counts),     # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Training\n",
        "# ------------------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    \"\"\"\n",
        "    x_hat, x: (B, C, T)\n",
        "    mask: (B, T)  (1=유효, 0=pad)\n",
        "    \"\"\"\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)          # (B,T)\n",
        "    mask_bc = mask.unsqueeze(1)                              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps                  # B*T_valid*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in ['loss', 'loss_rate', 'loss_recon', 'mae_rate', 'mae_count']}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)        # (B, C, T)\n",
        "        mask = batch[\"mask\"].to(device)     # (B, T)\n",
        "        y_count = batch[\"count\"].to(device) # (B,)\n",
        "        length = batch[\"length\"].to(device) # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat = model(x, mask)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss = loss_rate + lam_recon * loss_recon\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['mae_rate'] += torch.abs(rate_hat - y_rate).mean().item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v/n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Inference\n",
        "# ------------------------------------------------------------------------------\n",
        "def evaluation(model, trial_data, device, fs, gt_count=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_tensor = torch.tensor(trial_data, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        T = trial_data.shape[0]\n",
        "        duration = T / fs\n",
        "\n",
        "        rate_hat, _, _ = model(x_tensor, mask=None)   # (1,)\n",
        "        rate_val = rate_hat.item()\n",
        "        count_val = rate_val * duration\n",
        "\n",
        "    gt_str = f\"{gt_count:.2f}\" if gt_count is not None else \"Unknown\"\n",
        "    print(\"-\" * 50)\n",
        "    print(f\">>> Pred count: {count_val:.2f} | (rate={rate_val:.3f} reps/s, duration={duration:.2f}s) | GT: {gt_str}\")\n",
        "    print(\"-\" * 50)\n",
        "    return count_val\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7. Main with CONFIG\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z', 'acc_arm_x', 'acc_arm_y', 'acc_arm_z'],\n",
        "            7: ['acc_arm_x', 'acc_arm_y', 'acc_arm_z', 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n",
        "            8: ['acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z', 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z'],\n",
        "            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z', 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,  # Small batch size since sequences are long\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "\n",
        "        # Model Arch\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 3,\n",
        "\n",
        "        \"TRAIN_LABELS\": [\n",
        "            # (Subject_ID, Activity_ID, True_Count)\n",
        "            (\"subject1\", 12, 40),\n",
        "            (\"subject2\", 12, 45),\n",
        "            (\"subject3\", 12, 42),\n",
        "            (\"subject4\", 12, 42),\n",
        "            (\"subject5\", 12, 40),\n",
        "            (\"subject6\", 12, 42),\n",
        "            (\"subject7\", 12, 38),\n",
        "            (\"subject8\", 12, 41),\n",
        "            (\"subject9\", 12, 41),\n",
        "        ],\n",
        "\n",
        "        \"TEST_LABELS\": [\n",
        "            (\"subject10\", 12, 40),\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 1. Setup\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # 2. Load Raw Data\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "\n",
        "    if not full_data:\n",
        "        print(\"No data loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 3. Prepare Dataset\n",
        "    train_data = prepare_trial_list(\n",
        "        CONFIG[\"TRAIN_LABELS\"],\n",
        "        full_data,\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "    )\n",
        "\n",
        "    test_data = prepare_trial_list(\n",
        "        CONFIG[\"TEST_LABELS\"],\n",
        "        full_data,\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"ACT_FEATURE_MAP\"]\n",
        "    )\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"No training data found. Check CONFIG.\")\n",
        "        return\n",
        "\n",
        "    train_loader = DataLoader(TrialDataset(train_data),\n",
        "                              batch_size=CONFIG[\"batch_size\"],\n",
        "                              shuffle=True,\n",
        "                              collate_fn=collate_variable_length)\n",
        "\n",
        "    # 4. Model Init\n",
        "    # Input channels depends on the activity (e.g., Jump has 6 feats)\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = RateModel(input_ch=input_ch,\n",
        "                     hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "                     latent_dim=CONFIG[\"latent_dim\"]).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5) # 30에폭마다 학습률 절반으로\n",
        "\n",
        "    # 5. Training Loop\n",
        "    print(\"\\nStarting Training...\")\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "\n",
        "        res = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f\"Ep {epoch+1} | \"\n",
        "            f\"Loss: {res['loss']:.4f} | \"\n",
        "            f\"Rate: {res['loss_rate']:.4f} | \"\n",
        "            f\"Recon: {res['loss_recon']:.4f} | \"\n",
        "            f\"MAE_count: {res['mae_count']:.2f}\")\n",
        "\n",
        "    # 6. Test & Visualize\n",
        "    print(\"\\n[Inference on Test Set]\")\n",
        "    target_viz_id = 12\n",
        "    viz_feat_names = CONFIG[\"ACT_FEATURE_MAP\"].get(target_viz_id, [\"feat\"]*input_ch)\n",
        "\n",
        "    if len(test_data) > 0:\n",
        "        for item in test_data:\n",
        "            print(f\"Subject: {item['meta']} | GT Count: {item['count']}\")\n",
        "            evaluation(model, item['data'], device, fs=CONFIG[\"fs\"], gt_count=item['count'])\n",
        "    else:\n",
        "        print(\"No test data defined. Showing train sample.\")\n",
        "        item = train_data[0]\n",
        "        evaluation(model, item['data'], device, fs=CONFIG[\"fs\"], gt_count=item['count'])\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8. Stress Test\n",
        "# ------------------------------------------------------------------------------\n",
        "    def run_comprehensive_stress_test(model, trial_data, device, fs=50):\n",
        "        model.eval()\n",
        "        data_np = trial_data['data']\n",
        "        T, C = data_np.shape\n",
        "        dur_orig = T / fs\n",
        "\n",
        "        print(\"=\"*60)\n",
        "        print(f\"[Comprehensive Stress Test] Target: {trial_data['meta']}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 1. Length Invariance Test (Zero Padding)\n",
        "        pad_len = int(T * 0.5)\n",
        "        zeros = np.zeros((pad_len, C), dtype=np.float32)\n",
        "        padded_data = np.concatenate([data_np, zeros], axis=0)\n",
        "        T_pad = padded_data.shape[0]\n",
        "        dur_pad = T_pad / fs\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_orig = torch.tensor(data_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_orig, _, _ = model(x_orig, mask=None)\n",
        "            count_orig = rate_orig.item() * dur_orig\n",
        "\n",
        "            x_pad = torch.tensor(padded_data, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_pad, _, _ = model(x_pad, mask=None)\n",
        "            count_pad = rate_pad.item() * dur_pad\n",
        "\n",
        "        print(f\"1. [Length Invariance] Add 50% Silence\")\n",
        "        print(f\"   - Original | Len: {dur_orig:.2f}s | Rate: {rate_orig.item():.3f} | Count: {count_orig:.2f}\")\n",
        "        print(f\"   - Padded   | Len: {dur_pad:.2f}s | Rate: {rate_pad.item():.3f} | Count: {count_pad:.2f}\")\n",
        "\n",
        "        diff_len = abs(count_orig - count_pad)\n",
        "        pct_len = (diff_len / count_orig) * 100\n",
        "        if pct_len < 5.0:\n",
        "            print(f\"   ✅ PASS: Count maintained (Diff: {pct_len:.2f}%)\")\n",
        "        else:\n",
        "            print(f\"   ❌ WARNING: Count changed (Diff: {pct_len:.2f}%)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # 2. Consistency Test (Front vs Back Split)\n",
        "        mid_point = T // 2\n",
        "        front_np = data_np[:mid_point, :]\n",
        "        back_np = data_np[mid_point:, :]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_front = torch.tensor(front_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_front, _, _ = model(x_front,  mask=None)\n",
        "            rate_front = rate_front.item()\n",
        "\n",
        "            x_back = torch.tensor(back_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_back, _, _ = model(x_back,  mask=None)\n",
        "            rate_back = rate_back.item()\n",
        "\n",
        "        print(f\"2. [Consistency Test] Front vs Back Rate\")\n",
        "        print(f\"   - Full  (0~100%) : {rate_orig.item():.3f} reps/s\")\n",
        "        print(f\"   - Front (0~50%)  : {rate_front:.3f} reps/s\")\n",
        "        print(f\"   - Back  (50~100%): {rate_back:.3f} reps/s\")\n",
        "\n",
        "        diff_consist = abs(rate_front - rate_back) / (rate_orig.item() + 1e-6) * 100\n",
        "        if diff_consist < 15.0:\n",
        "            print(f\"   ✅ PASS: Rate is consistent (Diff: {diff_consist:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"   ⚠️ WARNING: Large rate difference (Diff: {diff_consist:.1f}%)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    run_comprehensive_stress_test(model, test_data[0], device, fs=CONFIG[\"fs\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnvaTL0-ndnZ",
        "outputId": "ea79827b-969d-494e-86f2-de7cb806adc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Seed fixed to 42\n",
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "Starting Training...\n",
            "Ep 10 | Loss: 1.6222 | Rate: 0.4711 | Recon: 1.1511 | MAE_count: 13.59\n",
            "Ep 20 | Loss: 0.9347 | Rate: 0.1474 | Recon: 0.7873 | MAE_count: 7.60\n",
            "Ep 30 | Loss: 0.6941 | Rate: 0.0670 | Recon: 0.6270 | MAE_count: 5.28\n",
            "Ep 40 | Loss: 0.5656 | Rate: 0.0034 | Recon: 0.5622 | MAE_count: 0.98\n",
            "Ep 50 | Loss: 0.5247 | Rate: 0.0054 | Recon: 0.5193 | MAE_count: 1.29\n",
            "Ep 60 | Loss: 0.4880 | Rate: 0.0023 | Recon: 0.4858 | MAE_count: 0.81\n",
            "Ep 70 | Loss: 0.4706 | Rate: 0.0007 | Recon: 0.4699 | MAE_count: 0.51\n",
            "Ep 80 | Loss: 0.4567 | Rate: 0.0006 | Recon: 0.4561 | MAE_count: 0.48\n",
            "Ep 90 | Loss: 0.4435 | Rate: 0.0006 | Recon: 0.4429 | MAE_count: 0.46\n",
            "Ep 100 | Loss: 0.4364 | Rate: 0.0006 | Recon: 0.4358 | MAE_count: 0.46\n",
            "\n",
            "[Inference on Test Set]\n",
            "Subject: subject10_Jump front & back | GT Count: 40.0\n",
            "--------------------------------------------------\n",
            ">>> Pred count: 38.88 | (rate=1.898 reps/s, duration=20.48s) | GT: 40.00\n",
            "--------------------------------------------------\n",
            "============================================================\n",
            "[Comprehensive Stress Test] Target: subject10_Jump front & back\n",
            "============================================================\n",
            "1. [Length Invariance] Add 50% Silence\n",
            "   - Original | Len: 20.48s | Rate: 1.898 | Count: 38.88\n",
            "   - Padded   | Len: 30.72s | Rate: 1.318 | Count: 40.49\n",
            "   ✅ PASS: Count maintained (Diff: 4.16%)\n",
            "------------------------------------------------------------\n",
            "2. [Consistency Test] Front vs Back Rate\n",
            "   - Full  (0~100%) : 1.898 reps/s\n",
            "   - Front (0~50%)  : 1.877 reps/s\n",
            "   - Back  (50~100%): 1.912 reps/s\n",
            "   ✅ PASS: Rate is consistent (Diff: 1.8%)\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}