{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI_BJKhLvmbU",
        "outputId": "bcd2fb8a-17c8-4c08-acb7-492653161555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act6)  ->  Test(1 subj × act7)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Frontal elevation of arms | Pred(win)=19.03 / GT=20.00 | MAE=0.97 | k_hat=1.04 | ent=0.094 | win_rate_mean=0.310\n",
            "[Fold 02] subject2_Frontal elevation of arms | Pred(win)=19.24 / GT=20.00 | MAE=0.76 | k_hat=1.02 | ent=0.053 | win_rate_mean=0.289\n",
            "[Fold 03] subject3_Frontal elevation of arms | Pred(win)=17.03 / GT=20.00 | MAE=2.97 | k_hat=1.17 | ent=0.218 | win_rate_mean=0.252\n",
            "[Fold 04] subject4_Frontal elevation of arms | Pred(win)=18.37 / GT=20.00 | MAE=1.63 | k_hat=1.05 | ent=0.110 | win_rate_mean=0.280\n",
            "[Fold 05] subject5_Frontal elevation of arms | Pred(win)=16.26 / GT=20.00 | MAE=3.74 | k_hat=1.03 | ent=0.079 | win_rate_mean=0.283\n",
            "[Fold 06] subject6_Frontal elevation of arms | Pred(win)=14.78 / GT=20.00 | MAE=5.22 | k_hat=1.02 | ent=0.066 | win_rate_mean=0.352\n",
            "[Fold 07] subject7_Frontal elevation of arms | Pred(win)=17.92 / GT=20.00 | MAE=2.08 | k_hat=1.07 | ent=0.155 | win_rate_mean=0.324\n",
            "[Fold 08] subject8_Frontal elevation of arms | Pred(win)=23.90 / GT=19.00 | MAE=4.90 | k_hat=2.04 | ent=0.423 | win_rate_mean=0.396\n",
            "[Fold 09] subject9_Frontal elevation of arms | Pred(win)=17.14 / GT=19.00 | MAE=1.86 | k_hat=1.08 | ent=0.169 | win_rate_mean=0.299\n",
            "[Fold 10] subject10_Frontal elevation of arms | Pred(win)=16.83 / GT=20.00 | MAE=3.17 | k_hat=1.02 | ent=0.043 | win_rate_mean=0.304\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 2.730\n",
            " >>> A2 Final MAE std : 1.469\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 6,   # A\n",
        "        \"TEST_ACT_ID\": 7,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 7: 20},\n",
        "            \"subject2\":  {6: 19, 7: 20},\n",
        "            \"subject3\":  {6: 21, 7: 20},\n",
        "            \"subject4\":  {6: 20, 7: 20},\n",
        "            \"subject5\":  {6: 20, 7: 20},\n",
        "            \"subject6\":  {6: 20, 7: 20},\n",
        "            \"subject7\":  {6: 20, 7: 20},\n",
        "            \"subject8\":  {6: 21, 7: 19},\n",
        "            \"subject9\":  {6: 21, 7: 19},\n",
        "            \"subject10\": {6: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 7,   # A\n",
        "        \"TEST_ACT_ID\": 6,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 7: 20},\n",
        "            \"subject2\":  {6: 19, 7: 20},\n",
        "            \"subject3\":  {6: 21, 7: 20},\n",
        "            \"subject4\":  {6: 20, 7: 20},\n",
        "            \"subject5\":  {6: 20, 7: 20},\n",
        "            \"subject6\":  {6: 20, 7: 20},\n",
        "            \"subject7\":  {6: 20, 7: 20},\n",
        "            \"subject8\":  {6: 21, 7: 19},\n",
        "            \"subject9\":  {6: 21, 7: 19},\n",
        "            \"subject10\": {6: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GauyWusyjeB",
        "outputId": "558a768f-2e48-4262-952b-db230d5134ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act7)  ->  Test(1 subj × act6)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Waist bends forward | Pred(win)=27.27 / GT=21.00 | MAE=6.27 | k_hat=1.01 | ent=0.026 | win_rate_mean=0.444\n",
            "[Fold 02] subject2_Waist bends forward | Pred(win)=17.65 / GT=19.00 | MAE=1.35 | k_hat=1.01 | ent=0.037 | win_rate_mean=0.278\n",
            "[Fold 03] subject3_Waist bends forward | Pred(win)=18.32 / GT=21.00 | MAE=2.68 | k_hat=1.96 | ent=0.375 | win_rate_mean=0.284\n",
            "[Fold 04] subject4_Waist bends forward | Pred(win)=23.27 / GT=20.00 | MAE=3.27 | k_hat=1.02 | ent=0.054 | win_rate_mean=0.350\n",
            "[Fold 05] subject5_Waist bends forward | Pred(win)=21.88 / GT=20.00 | MAE=1.88 | k_hat=1.01 | ent=0.033 | win_rate_mean=0.396\n",
            "[Fold 06] subject6_Waist bends forward | Pred(win)=16.09 / GT=20.00 | MAE=3.91 | k_hat=1.04 | ent=0.091 | win_rate_mean=0.365\n",
            "[Fold 07] subject7_Waist bends forward | Pred(win)=16.94 / GT=20.00 | MAE=3.06 | k_hat=1.02 | ent=0.045 | win_rate_mean=0.276\n",
            "[Fold 08] subject8_Waist bends forward | Pred(win)=12.54 / GT=21.00 | MAE=8.46 | k_hat=1.04 | ent=0.082 | win_rate_mean=0.292\n",
            "[Fold 09] subject9_Waist bends forward | Pred(win)=13.12 / GT=21.00 | MAE=7.88 | k_hat=1.02 | ent=0.052 | win_rate_mean=0.229\n",
            "[Fold 10] subject10_Waist bends forward | Pred(win)=13.02 / GT=20.00 | MAE=6.98 | k_hat=1.04 | ent=0.082 | win_rate_mean=0.265\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 4.575\n",
            " >>> A2 Final MAE std : 2.456\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 6,   # A\n",
        "        \"TEST_ACT_ID\": 8,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 8: 20},\n",
        "            \"subject2\":  {6: 19, 8: 21},\n",
        "            \"subject3\":  {6: 21, 8: 21},\n",
        "            \"subject4\":  {6: 20, 8: 19},\n",
        "            \"subject5\":  {6: 20, 8: 20},\n",
        "            \"subject6\":  {6: 20, 8: 20},\n",
        "            \"subject7\":  {6: 20, 8: 21},\n",
        "            \"subject8\":  {6: 21, 8: 21},\n",
        "            \"subject9\":  {6: 21, 8: 21},\n",
        "            \"subject10\": {6: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf-vtlVtzRAt",
        "outputId": "017a93f3-8032-468a-f569-39dd063914f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act6)  ->  Test(1 subj × act8)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Knees bending | Pred(win)=17.93 / GT=20.00 | MAE=2.07 | k_hat=1.09 | ent=0.178 | win_rate_mean=0.265\n",
            "[Fold 02] subject2_Knees bending | Pred(win)=28.23 / GT=21.00 | MAE=7.23 | k_hat=1.05 | ent=0.119 | win_rate_mean=0.411\n",
            "[Fold 03] subject3_Knees bending | Pred(win)=17.36 / GT=21.00 | MAE=3.64 | k_hat=1.08 | ent=0.141 | win_rate_mean=0.273\n",
            "[Fold 04] subject4_Knees bending | Pred(win)=13.38 / GT=19.00 | MAE=5.62 | k_hat=1.08 | ent=0.175 | win_rate_mean=0.214\n",
            "[Fold 05] subject5_Knees bending | Pred(win)=20.86 / GT=20.00 | MAE=0.86 | k_hat=1.01 | ent=0.034 | win_rate_mean=0.384\n",
            "[Fold 06] subject6_Knees bending | Pred(win)=10.38 / GT=20.00 | MAE=9.62 | k_hat=1.04 | ent=0.098 | win_rate_mean=0.225\n",
            "[Fold 07] subject7_Knees bending | Pred(win)=20.42 / GT=21.00 | MAE=0.58 | k_hat=1.11 | ent=0.228 | win_rate_mean=0.363\n",
            "[Fold 08] subject8_Knees bending | Pred(win)=15.75 / GT=21.00 | MAE=5.25 | k_hat=2.09 | ent=0.318 | win_rate_mean=0.308\n",
            "[Fold 09] subject9_Knees bending | Pred(win)=16.86 / GT=21.00 | MAE=4.14 | k_hat=1.06 | ent=0.137 | win_rate_mean=0.284\n",
            "[Fold 10] subject10_Knees bending | Pred(win)=24.18 / GT=21.00 | MAE=3.18 | k_hat=1.01 | ent=0.040 | win_rate_mean=0.422\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 4.219\n",
            " >>> A2 Final MAE std : 2.679\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 8,   # A\n",
        "        \"TEST_ACT_ID\": 6,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 8: 20},\n",
        "            \"subject2\":  {6: 19, 8: 21},\n",
        "            \"subject3\":  {6: 21, 8: 21},\n",
        "            \"subject4\":  {6: 20, 8: 19},\n",
        "            \"subject5\":  {6: 20, 8: 20},\n",
        "            \"subject6\":  {6: 20, 8: 20},\n",
        "            \"subject7\":  {6: 20, 8: 21},\n",
        "            \"subject8\":  {6: 21, 8: 21},\n",
        "            \"subject9\":  {6: 21, 8: 21},\n",
        "            \"subject10\": {6: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZC6FI03zzFB",
        "outputId": "f639cd26-4492-40c5-c029-fa7feda6bf1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act8)  ->  Test(1 subj × act6)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Waist bends forward | Pred(win)=15.90 / GT=21.00 | MAE=5.10 | k_hat=1.01 | ent=0.041 | win_rate_mean=0.259\n",
            "[Fold 02] subject2_Waist bends forward | Pred(win)=17.40 / GT=19.00 | MAE=1.60 | k_hat=1.03 | ent=0.068 | win_rate_mean=0.274\n",
            "[Fold 03] subject3_Waist bends forward | Pred(win)=12.61 / GT=21.00 | MAE=8.39 | k_hat=1.06 | ent=0.124 | win_rate_mean=0.195\n",
            "[Fold 04] subject4_Waist bends forward | Pred(win)=27.18 / GT=20.00 | MAE=7.18 | k_hat=1.05 | ent=0.107 | win_rate_mean=0.408\n",
            "[Fold 05] subject5_Waist bends forward | Pred(win)=14.68 / GT=20.00 | MAE=5.32 | k_hat=1.03 | ent=0.075 | win_rate_mean=0.265\n",
            "[Fold 06] subject6_Waist bends forward | Pred(win)=11.18 / GT=20.00 | MAE=8.82 | k_hat=2.08 | ent=0.354 | win_rate_mean=0.254\n",
            "[Fold 07] subject7_Waist bends forward | Pred(win)=13.27 / GT=20.00 | MAE=6.73 | k_hat=1.06 | ent=0.135 | win_rate_mean=0.216\n",
            "[Fold 08] subject8_Waist bends forward | Pred(win)=8.95 / GT=21.00 | MAE=12.05 | k_hat=1.77 | ent=0.288 | win_rate_mean=0.208\n",
            "[Fold 09] subject9_Waist bends forward | Pred(win)=15.10 / GT=21.00 | MAE=5.90 | k_hat=1.05 | ent=0.103 | win_rate_mean=0.263\n",
            "[Fold 10] subject10_Waist bends forward | Pred(win)=9.44 / GT=20.00 | MAE=10.56 | k_hat=1.07 | ent=0.134 | win_rate_mean=0.192\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 7.167\n",
            " >>> A2 Final MAE std : 2.830\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 7,   # A\n",
        "        \"TEST_ACT_ID\": 8,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {7: 20, 8: 20},\n",
        "            \"subject2\":  {7: 20, 8: 21},\n",
        "            \"subject3\":  {7: 20, 8: 21},\n",
        "            \"subject4\":  {7: 20, 8: 19},\n",
        "            \"subject5\":  {7: 20, 8: 20},\n",
        "            \"subject6\":  {7: 20, 8: 20},\n",
        "            \"subject7\":  {7: 20, 8: 21},\n",
        "            \"subject8\":  {7: 19, 8: 21},\n",
        "            \"subject9\":  {7: 19, 8: 21},\n",
        "            \"subject10\": {7: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl1dmm7uz_Ec",
        "outputId": "cde3e9fa-bd05-4f9f-b46f-349c350b066f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act7)  ->  Test(1 subj × act8)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Knees bending | Pred(win)=21.02 / GT=20.00 | MAE=1.02 | k_hat=1.06 | ent=0.137 | win_rate_mean=0.311\n",
            "[Fold 02] subject2_Knees bending | Pred(win)=22.99 / GT=21.00 | MAE=1.99 | k_hat=1.03 | ent=0.074 | win_rate_mean=0.335\n",
            "[Fold 03] subject3_Knees bending | Pred(win)=22.21 / GT=21.00 | MAE=1.21 | k_hat=2.03 | ent=0.352 | win_rate_mean=0.350\n",
            "[Fold 04] subject4_Knees bending | Pred(win)=18.72 / GT=19.00 | MAE=0.28 | k_hat=1.03 | ent=0.065 | win_rate_mean=0.300\n",
            "[Fold 05] subject5_Knees bending | Pred(win)=22.34 / GT=20.00 | MAE=2.34 | k_hat=1.01 | ent=0.036 | win_rate_mean=0.412\n",
            "[Fold 06] subject6_Knees bending | Pred(win)=12.27 / GT=20.00 | MAE=7.73 | k_hat=1.06 | ent=0.128 | win_rate_mean=0.266\n",
            "[Fold 07] subject7_Knees bending | Pred(win)=16.13 / GT=21.00 | MAE=4.87 | k_hat=1.10 | ent=0.179 | win_rate_mean=0.286\n",
            "[Fold 08] subject8_Knees bending | Pred(win)=11.83 / GT=21.00 | MAE=9.17 | k_hat=1.04 | ent=0.088 | win_rate_mean=0.231\n",
            "[Fold 09] subject9_Knees bending | Pred(win)=21.53 / GT=21.00 | MAE=0.53 | k_hat=1.02 | ent=0.049 | win_rate_mean=0.363\n",
            "[Fold 10] subject10_Knees bending | Pred(win)=20.45 / GT=21.00 | MAE=0.55 | k_hat=1.02 | ent=0.057 | win_rate_mean=0.357\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 2.969\n",
            " >>> A2 Final MAE std : 3.034\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 8,   # A\n",
        "        \"TEST_ACT_ID\": 7,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {7: 20, 8: 20},\n",
        "            \"subject2\":  {7: 20, 8: 21},\n",
        "            \"subject3\":  {7: 20, 8: 21},\n",
        "            \"subject4\":  {7: 20, 8: 19},\n",
        "            \"subject5\":  {7: 20, 8: 20},\n",
        "            \"subject6\":  {7: 20, 8: 20},\n",
        "            \"subject7\":  {7: 20, 8: 21},\n",
        "            \"subject8\":  {7: 19, 8: 21},\n",
        "            \"subject9\":  {7: 19, 8: 21},\n",
        "            \"subject10\": {7: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YXGLUT60RUi",
        "outputId": "00bd1fff-f4e8-435c-c2a8-fdc8a8db0dc1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act8)  ->  Test(1 subj × act7)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Frontal elevation of arms | Pred(win)=16.27 / GT=20.00 | MAE=3.73 | k_hat=1.02 | ent=0.049 | win_rate_mean=0.265\n",
            "[Fold 02] subject2_Frontal elevation of arms | Pred(win)=22.53 / GT=20.00 | MAE=2.53 | k_hat=1.02 | ent=0.047 | win_rate_mean=0.339\n",
            "[Fold 03] subject3_Frontal elevation of arms | Pred(win)=15.39 / GT=20.00 | MAE=4.61 | k_hat=1.12 | ent=0.197 | win_rate_mean=0.228\n",
            "[Fold 04] subject4_Frontal elevation of arms | Pred(win)=20.18 / GT=20.00 | MAE=0.18 | k_hat=1.03 | ent=0.076 | win_rate_mean=0.308\n",
            "[Fold 05] subject5_Frontal elevation of arms | Pred(win)=14.58 / GT=20.00 | MAE=5.42 | k_hat=1.05 | ent=0.114 | win_rate_mean=0.254\n",
            "[Fold 06] subject6_Frontal elevation of arms | Pred(win)=10.35 / GT=20.00 | MAE=9.65 | k_hat=2.00 | ent=0.451 | win_rate_mean=0.247\n",
            "[Fold 07] subject7_Frontal elevation of arms | Pred(win)=10.13 / GT=20.00 | MAE=9.87 | k_hat=1.10 | ent=0.191 | win_rate_mean=0.183\n",
            "[Fold 08] subject8_Frontal elevation of arms | Pred(win)=18.83 / GT=19.00 | MAE=0.17 | k_hat=1.52 | ent=0.329 | win_rate_mean=0.312\n",
            "[Fold 09] subject9_Frontal elevation of arms | Pred(win)=16.29 / GT=19.00 | MAE=2.71 | k_hat=1.06 | ent=0.125 | win_rate_mean=0.284\n",
            "[Fold 10] subject10_Frontal elevation of arms | Pred(win)=13.39 / GT=20.00 | MAE=6.61 | k_hat=1.03 | ent=0.067 | win_rate_mean=0.242\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 4.549\n",
            " >>> A2 Final MAE std : 3.251\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 8,   # A\n",
        "        \"TEST_ACT_ID\": 12,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 8: 20},\n",
        "            \"subject2\":  {12: 22, 8: 21},\n",
        "            \"subject3\":  {12: 21, 8: 21},\n",
        "            \"subject4\":  {12: 21, 8: 19},\n",
        "            \"subject5\":  {12: 20, 8: 20},\n",
        "            \"subject6\":  {12: 21, 8: 20},\n",
        "            \"subject7\":  {12: 19, 8: 21},\n",
        "            \"subject8\":  {12: 20, 8: 21},\n",
        "            \"subject9\":  {12: 20, 8: 21},\n",
        "            \"subject10\": {12: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkmU049f0c-u",
        "outputId": "7b2f77d5-29dc-436d-95a3-4edcaab33e83"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act8)  ->  Test(1 subj × act12)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Jump front & back | Pred(win)=6.03 / GT=20.00 | MAE=13.97 | k_hat=1.02 | ent=0.049 | win_rate_mean=0.281\n",
            "[Fold 02] subject2_Jump front & back | Pred(win)=6.52 / GT=22.00 | MAE=15.48 | k_hat=1.02 | ent=0.055 | win_rate_mean=0.319\n",
            "[Fold 03] subject3_Jump front & back | Pred(win)=4.75 / GT=21.00 | MAE=16.25 | k_hat=1.07 | ent=0.137 | win_rate_mean=0.232\n",
            "[Fold 04] subject4_Jump front & back | Pred(win)=6.93 / GT=21.00 | MAE=14.07 | k_hat=1.05 | ent=0.110 | win_rate_mean=0.339\n",
            "[Fold 05] subject5_Jump front & back | Pred(win)=5.60 / GT=20.00 | MAE=14.40 | k_hat=1.10 | ent=0.203 | win_rate_mean=0.273\n",
            "[Fold 06] subject6_Jump front & back | Pred(win)=5.08 / GT=21.00 | MAE=15.92 | k_hat=2.04 | ent=0.488 | win_rate_mean=0.248\n",
            "[Fold 07] subject7_Jump front & back | Pred(win)=3.05 / GT=19.00 | MAE=15.95 | k_hat=1.13 | ent=0.248 | win_rate_mean=0.149\n",
            "[Fold 08] subject8_Jump front & back | Pred(win)=4.01 / GT=20.00 | MAE=15.99 | k_hat=1.85 | ent=0.366 | win_rate_mean=0.196\n",
            "[Fold 09] subject9_Jump front & back | Pred(win)=4.92 / GT=20.00 | MAE=15.08 | k_hat=1.06 | ent=0.130 | win_rate_mean=0.229\n",
            "[Fold 10] subject10_Jump front & back | Pred(win)=6.59 / GT=20.00 | MAE=13.41 | k_hat=1.03 | ent=0.074 | win_rate_mean=0.322\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 15.051\n",
            " >>> A2 Final MAE std : 0.966\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 12,   # A\n",
        "        \"TEST_ACT_ID\": 8,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 8: 20},\n",
        "            \"subject2\":  {12: 22, 8: 21},\n",
        "            \"subject3\":  {12: 21, 8: 21},\n",
        "            \"subject4\":  {12: 21, 8: 19},\n",
        "            \"subject5\":  {12: 20, 8: 20},\n",
        "            \"subject6\":  {12: 21, 8: 20},\n",
        "            \"subject7\":  {12: 19, 8: 21},\n",
        "            \"subject8\":  {12: 20, 8: 21},\n",
        "            \"subject9\":  {12: 20, 8: 21},\n",
        "            \"subject10\": {12: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOq1kH5a0tQg",
        "outputId": "124df5a1-e2b4-42aa-9a2e-74017008af4e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act12)  ->  Test(1 subj × act8)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Knees bending | Pred(win)=51.99 / GT=20.00 | MAE=31.99 | k_hat=1.39 | ent=0.529 | win_rate_mean=0.769\n",
            "[Fold 02] subject2_Knees bending | Pred(win)=61.69 / GT=21.00 | MAE=40.69 | k_hat=1.17 | ent=0.312 | win_rate_mean=0.899\n",
            "[Fold 03] subject3_Knees bending | Pred(win)=74.61 / GT=21.00 | MAE=53.61 | k_hat=1.94 | ent=0.682 | win_rate_mean=1.175\n",
            "[Fold 04] subject4_Knees bending | Pred(win)=73.63 / GT=19.00 | MAE=54.63 | k_hat=2.72 | ent=0.988 | win_rate_mean=1.179\n",
            "[Fold 05] subject5_Knees bending | Pred(win)=65.59 / GT=20.00 | MAE=45.59 | k_hat=1.34 | ent=0.449 | win_rate_mean=1.208\n",
            "[Fold 06] subject6_Knees bending | Pred(win)=37.39 / GT=20.00 | MAE=17.39 | k_hat=1.94 | ent=0.781 | win_rate_mean=0.811\n",
            "[Fold 07] subject7_Knees bending | Pred(win)=50.62 / GT=21.00 | MAE=29.62 | k_hat=3.39 | ent=1.078 | win_rate_mean=0.899\n",
            "[Fold 08] subject8_Knees bending | Pred(win)=43.54 / GT=21.00 | MAE=22.54 | k_hat=2.44 | ent=0.863 | win_rate_mean=0.850\n",
            "[Fold 09] subject9_Knees bending | Pred(win)=49.80 / GT=21.00 | MAE=28.80 | k_hat=1.21 | ent=0.347 | win_rate_mean=0.839\n",
            "[Fold 10] subject10_Knees bending | Pred(win)=60.50 / GT=21.00 | MAE=39.50 | k_hat=1.19 | ent=0.342 | win_rate_mean=1.055\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 36.437\n",
            " >>> A2 Final MAE std : 11.907\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 6,   # A\n",
        "        \"TEST_ACT_ID\": 12,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 6: 21},\n",
        "            \"subject2\":  {12: 22, 6: 19},\n",
        "            \"subject3\":  {12: 21, 6: 21},\n",
        "            \"subject4\":  {12: 21, 6: 20},\n",
        "            \"subject5\":  {12: 20, 6: 20},\n",
        "            \"subject6\":  {12: 21, 6: 20},\n",
        "            \"subject7\":  {12: 19, 6: 20},\n",
        "            \"subject8\":  {12: 20, 6: 21},\n",
        "            \"subject9\":  {12: 20, 6: 21},\n",
        "            \"subject10\": {12: 20, 6: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VODsO2T01cr",
        "outputId": "2f962f89-51fa-4413-d992-fcee47be38da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act6)  ->  Test(1 subj × act12)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Jump front & back | Pred(win)=4.82 / GT=20.00 | MAE=15.18 | k_hat=1.04 | ent=0.097 | win_rate_mean=0.224\n",
            "[Fold 02] subject2_Jump front & back | Pred(win)=7.68 / GT=22.00 | MAE=14.32 | k_hat=1.02 | ent=0.041 | win_rate_mean=0.375\n",
            "[Fold 03] subject3_Jump front & back | Pred(win)=4.57 / GT=21.00 | MAE=16.43 | k_hat=1.09 | ent=0.182 | win_rate_mean=0.223\n",
            "[Fold 04] subject4_Jump front & back | Pred(win)=5.72 / GT=21.00 | MAE=15.28 | k_hat=1.13 | ent=0.238 | win_rate_mean=0.279\n",
            "[Fold 05] subject5_Jump front & back | Pred(win)=5.72 / GT=20.00 | MAE=14.28 | k_hat=1.06 | ent=0.135 | win_rate_mean=0.279\n",
            "[Fold 06] subject6_Jump front & back | Pred(win)=6.47 / GT=21.00 | MAE=14.53 | k_hat=1.04 | ent=0.090 | win_rate_mean=0.316\n",
            "[Fold 07] subject7_Jump front & back | Pred(win)=7.28 / GT=19.00 | MAE=11.72 | k_hat=1.05 | ent=0.127 | win_rate_mean=0.356\n",
            "[Fold 08] subject8_Jump front & back | Pred(win)=6.14 / GT=20.00 | MAE=13.86 | k_hat=2.05 | ent=0.397 | win_rate_mean=0.300\n",
            "[Fold 09] subject9_Jump front & back | Pred(win)=6.68 / GT=20.00 | MAE=13.32 | k_hat=1.12 | ent=0.232 | win_rate_mean=0.311\n",
            "[Fold 10] subject10_Jump front & back | Pred(win)=8.48 / GT=20.00 | MAE=11.52 | k_hat=1.02 | ent=0.051 | win_rate_mean=0.414\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 14.044\n",
            " >>> A2 Final MAE std : 1.457\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 12,   # A\n",
        "        \"TEST_ACT_ID\": 6,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 6: 21},\n",
        "            \"subject2\":  {12: 22, 6: 19},\n",
        "            \"subject3\":  {12: 21, 6: 21},\n",
        "            \"subject4\":  {12: 21, 6: 20},\n",
        "            \"subject5\":  {12: 20, 6: 20},\n",
        "            \"subject6\":  {12: 21, 6: 20},\n",
        "            \"subject7\":  {12: 19, 6: 20},\n",
        "            \"subject8\":  {12: 20, 6: 21},\n",
        "            \"subject9\":  {12: 20, 6: 21},\n",
        "            \"subject10\": {12: 20, 6: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai5bGTgE1Lqe",
        "outputId": "be1e8bc7-5c40-4060-8ba1-69b57b56fa6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act12)  ->  Test(1 subj × act6)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Waist bends forward | Pred(win)=83.05 / GT=21.00 | MAE=62.05 | k_hat=1.27 | ent=0.371 | win_rate_mean=1.352\n",
            "[Fold 02] subject2_Waist bends forward | Pred(win)=48.02 / GT=19.00 | MAE=29.02 | k_hat=1.13 | ent=0.254 | win_rate_mean=0.756\n",
            "[Fold 03] subject3_Waist bends forward | Pred(win)=70.21 / GT=21.00 | MAE=49.21 | k_hat=2.02 | ent=0.711 | win_rate_mean=1.088\n",
            "[Fold 04] subject4_Waist bends forward | Pred(win)=66.30 / GT=20.00 | MAE=46.30 | k_hat=3.15 | ent=1.066 | win_rate_mean=0.996\n",
            "[Fold 05] subject5_Waist bends forward | Pred(win)=49.83 / GT=20.00 | MAE=29.83 | k_hat=1.50 | ent=0.575 | win_rate_mean=0.901\n",
            "[Fold 06] subject6_Waist bends forward | Pred(win)=50.11 / GT=20.00 | MAE=30.11 | k_hat=1.53 | ent=0.619 | win_rate_mean=1.138\n",
            "[Fold 07] subject7_Waist bends forward | Pred(win)=70.98 / GT=20.00 | MAE=50.98 | k_hat=2.97 | ent=0.910 | win_rate_mean=1.155\n",
            "[Fold 08] subject8_Waist bends forward | Pred(win)=44.53 / GT=21.00 | MAE=23.53 | k_hat=1.93 | ent=0.687 | win_rate_mean=1.035\n",
            "[Fold 09] subject9_Waist bends forward | Pred(win)=58.75 / GT=21.00 | MAE=37.75 | k_hat=1.12 | ent=0.230 | win_rate_mean=1.025\n",
            "[Fold 10] subject10_Waist bends forward | Pred(win)=38.07 / GT=20.00 | MAE=18.07 | k_hat=1.22 | ent=0.362 | win_rate_mean=0.774\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 37.686\n",
            " >>> A2 Final MAE std : 13.253\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 7,   # A\n",
        "        \"TEST_ACT_ID\": 12,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 7: 20},\n",
        "            \"subject2\":  {12: 22, 7: 20},\n",
        "            \"subject3\":  {12: 21, 7: 20},\n",
        "            \"subject4\":  {12: 21, 7: 20},\n",
        "            \"subject5\":  {12: 20, 7: 20},\n",
        "            \"subject6\":  {12: 21, 7: 20},\n",
        "            \"subject7\":  {12: 19, 7: 20},\n",
        "            \"subject8\":  {12: 20, 7: 19},\n",
        "            \"subject9\":  {12: 20, 7: 19},\n",
        "            \"subject10\": {12: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYsHjQLE1Zdb",
        "outputId": "dc7e12d6-3279-470a-9388-ef0313a9b2e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act7)  ->  Test(1 subj × act12)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Jump front & back | Pred(win)=7.13 / GT=20.00 | MAE=12.87 | k_hat=1.01 | ent=0.023 | win_rate_mean=0.332\n",
            "[Fold 02] subject2_Jump front & back | Pred(win)=7.77 / GT=22.00 | MAE=14.23 | k_hat=1.01 | ent=0.014 | win_rate_mean=0.380\n",
            "[Fold 03] subject3_Jump front & back | Pred(win)=8.58 / GT=21.00 | MAE=12.42 | k_hat=1.99 | ent=0.251 | win_rate_mean=0.419\n",
            "[Fold 04] subject4_Jump front & back | Pred(win)=7.53 / GT=21.00 | MAE=13.47 | k_hat=1.08 | ent=0.127 | win_rate_mean=0.368\n",
            "[Fold 05] subject5_Jump front & back | Pred(win)=8.00 / GT=20.00 | MAE=12.00 | k_hat=1.02 | ent=0.067 | win_rate_mean=0.391\n",
            "[Fold 06] subject6_Jump front & back | Pred(win)=6.23 / GT=21.00 | MAE=14.77 | k_hat=1.05 | ent=0.101 | win_rate_mean=0.304\n",
            "[Fold 07] subject7_Jump front & back | Pred(win)=6.07 / GT=19.00 | MAE=12.93 | k_hat=1.08 | ent=0.147 | win_rate_mean=0.296\n",
            "[Fold 08] subject8_Jump front & back | Pred(win)=6.01 / GT=20.00 | MAE=13.99 | k_hat=1.02 | ent=0.040 | win_rate_mean=0.293\n",
            "[Fold 09] subject9_Jump front & back | Pred(win)=6.70 / GT=20.00 | MAE=13.30 | k_hat=1.03 | ent=0.079 | win_rate_mean=0.312\n",
            "[Fold 10] subject10_Jump front & back | Pred(win)=7.60 / GT=20.00 | MAE=12.40 | k_hat=1.06 | ent=0.137 | win_rate_mean=0.371\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 13.238\n",
            " >>> A2 Final MAE std : 0.841\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# ✅ A2 (LOSO + Unseen Activity)\n",
        "# - Fold마다:\n",
        "#   Train: 9명(subjects \\ test_subj) × Activity A (TRAIN_ACT_ID)\n",
        "#   Test : 1명(test_subj) × Activity B (TEST_ACT_ID)\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (샘플 단위 표준화)\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    window 라벨은 'trial-level rate'로 window count를 구성:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur\n",
        "\n",
        "        if T < win_len:\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        micro_rate_t = amp_t\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)  # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)\n",
        "    se = (x_hat - x) ** 2\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)\n",
        "        mask = batch[\"mask\"].to(device)\n",
        "        y_count = batch[\"count\"].to(device)\n",
        "        length = batch[\"length\"].to(device)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)\n",
        "        y_rate = y_count / duration\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count_hat = rate_hat * duration\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = max(len(loader), 1)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(phase_p_np, fs, title=\"phase_p heatmap + dominant phase\", max_T=2000):\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    dom = np.argmax(phase_ds, axis=1)\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T, aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :], aspect=\"auto\", origin=\"lower\", interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization\"):\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main: A2 (LOSO + Unseen Activity)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "\n",
        "        # ✅ 여기서 A(학습) / B(테스트) activity를 넣으면 됨\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.005,\n",
        "\n",
        "        # temperature\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # ✅ A2 setting\n",
        "        \"TRAIN_ACT_ID\": 12,   # A\n",
        "        \"TEST_ACT_ID\": 7,    # B\n",
        "\n",
        "        # Windowing\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # (시각화 너무 많으면 부담이라 기본 False 추천)\n",
        "        \"PLOT_EACH_FOLD\": False,\n",
        "        \"PLOT_LAST_FOLD\": False,\n",
        "\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 7: 20},\n",
        "            \"subject2\":  {12: 22, 7: 20},\n",
        "            \"subject3\":  {12: 21, 7: 20},\n",
        "            \"subject4\":  {12: 21, 7: 20},\n",
        "            \"subject5\":  {12: 20, 7: 20},\n",
        "            \"subject6\":  {12: 21, 7: 20},\n",
        "            \"subject7\":  {12: 19, 7: 20},\n",
        "            \"subject8\":  {12: 20, 7: 19},\n",
        "            \"subject9\":  {12: 20, 7: 19},\n",
        "            \"subject10\": {12: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    A = CONFIG[\"TRAIN_ACT_ID\"]\n",
        "    B = CONFIG[\"TEST_ACT_ID\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(f\" >>> A2 (LOSO + Unseen Activity): Train(9 subj × act{A})  ->  Test(1 subj × act{B})\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    fold_results = []\n",
        "    all_maes = []\n",
        "\n",
        "    for fold_i, test_subj in enumerate(subjects, start=1):\n",
        "        # fold마다 seed 살짝 바꿔서(완전 동일 반복 방지)도 되고, 싫으면 아래 줄을 CONFIG[\"seed\"]로 고정해도 됨\n",
        "        set_strict_seed(CONFIG[\"seed\"] + fold_i)\n",
        "\n",
        "        train_subjects = [s for s in subjects if s != test_subj]\n",
        "\n",
        "        train_labels = build_label_tuples_from_table(train_subjects, A, CONFIG[\"COUNT_TABLE\"])\n",
        "        test_labels  = build_label_tuples_from_table([test_subj], B, CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "        if len(train_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No train labels (act{A})\")\n",
        "            continue\n",
        "        if len(test_labels) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] No test labels (act{B}) for {test_subj}\")\n",
        "            continue\n",
        "\n",
        "        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "        if len(train_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] train_data empty.\")\n",
        "            continue\n",
        "        if len(test_data) == 0:\n",
        "            print(f\"[Fold {fold_i:02d}] [Skip] test_data empty.\")\n",
        "            continue\n",
        "\n",
        "        # Windowing train only\n",
        "        if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "            train_windows = trial_list_to_windows(\n",
        "                train_data,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                drop_last=CONFIG[\"DROP_LAST\"],\n",
        "            )\n",
        "            train_data_for_loader = train_windows\n",
        "        else:\n",
        "            train_data_for_loader = train_data\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            TrialDataset(train_data_for_loader),\n",
        "            batch_size=CONFIG[\"batch_size\"],\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_variable_length,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        input_ch = train_data[0]['data'].shape[1]\n",
        "        model = KAutoCountModel(\n",
        "            input_ch=input_ch,\n",
        "            hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "            latent_dim=CONFIG[\"latent_dim\"],\n",
        "            K_max=CONFIG[\"K_max\"]\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "        # print(\"\\n\" + \"-\" * 90)\n",
        "        # print(f\"[Fold {fold_i:02d}] Test subj = {test_subj} | Train: 9 subj × act{A} | Test: {test_subj} × act{B}\")\n",
        "        # print(\"-\" * 90)\n",
        "\n",
        "        for epoch in range(CONFIG[\"epochs\"]):\n",
        "            _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "            scheduler.step()\n",
        "\n",
        "        # Test (해당 fold는 test_subj 1명×actB라 보통 1개 trial)\n",
        "        model.eval()\n",
        "        viz_cache = []\n",
        "\n",
        "        for item in test_data:\n",
        "            x_np = item[\"data\"]\n",
        "            gt_count = float(item[\"count\"])\n",
        "\n",
        "            pred_count, win_rates = predict_count_by_windowing(\n",
        "                model,\n",
        "                x_np=x_np,\n",
        "                fs=CONFIG[\"fs\"],\n",
        "                win_sec=CONFIG[\"WIN_SEC\"],\n",
        "                stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "                device=device,\n",
        "                tau=CONFIG[\"tau\"],\n",
        "                batch_size=CONFIG[\"batch_size\"]\n",
        "            )\n",
        "\n",
        "            mae = abs(pred_count - gt_count)\n",
        "            all_maes.append(mae)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "                _, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "                k_hat = float(aux[\"k_hat\"].item())\n",
        "                ent = compute_phase_entropy_mean(phase_p)\n",
        "                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "                T = rep_rate.shape[0]\n",
        "                t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "            fold_results.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": test_subj,\n",
        "                \"test_meta\": item[\"meta\"],\n",
        "                \"pred\": float(pred_count),\n",
        "                \"gt\": float(gt_count),\n",
        "                \"mae\": float(mae),\n",
        "                \"k_hat\": float(k_hat),\n",
        "                \"entropy\": float(ent),\n",
        "                \"win_rate_mean\": float(win_rates.mean()) if win_rates is not None else np.nan,\n",
        "            })\n",
        "\n",
        "            viz_cache.append({\n",
        "                \"fold\": fold_i,\n",
        "                \"test_subj\": item[\"meta\"],\n",
        "                \"t\": t,\n",
        "                \"rep_rate\": rep_rate,\n",
        "                \"gt\": gt_count,\n",
        "                \"pred\": float(pred_count),\n",
        "                \"diff\": float(pred_count - gt_count),\n",
        "                \"k_hat\": k_hat,\n",
        "                \"entropy\": ent,\n",
        "                \"phase_p\": phase_p,\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"[Fold {fold_i:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "                f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "        # (선택) 시각화\n",
        "        # if CONFIG.get(\"PLOT_EACH_FOLD\", False) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 Fold {fold_i:02d} visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"Fold {fold_i:02d} | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "        # if CONFIG.get(\"PLOT_LAST_FOLD\", False) and (fold_i == len(subjects)) and len(viz_cache) > 0:\n",
        "        #     plot_folds_test_subplot(viz_cache, fs=CONFIG[\"fs\"], title=f\"A2 LAST FOLD visualization\")\n",
        "        #     for item in viz_cache:\n",
        "        #         plot_phase_heatmap_and_dominant(\n",
        "        #             item[\"phase_p\"], fs=CONFIG[\"fs\"],\n",
        "        #             title=f\"LAST | {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "        #             max_T=2000\n",
        "        #         )\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    if len(all_maes) > 0:\n",
        "        print(f\" >>> A2 Final MAE mean: {np.mean(all_maes):.3f}\")\n",
        "        print(f\" >>> A2 Final MAE std : {np.std(all_maes):.3f}\")\n",
        "        print(f\" >>> Num folds evaluated: {len(all_maes)}\")\n",
        "    else:\n",
        "        print(\" >>> No folds evaluated (check data / COUNT_TABLE / activity ids).\")\n",
        "    print(\"=\" * 90)\n",
        "\n",
        "    # fold_results를 보고 싶으면 여기서 DataFrame으로 출력/저장도 가능\n",
        "    # df_res = pd.DataFrame(fold_results)\n",
        "    # print(df_res)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQHdkwbU1wnm",
        "outputId": "3a687e47-2bc1-4476-dc60-4f3c18a0b038"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 (LOSO + Unseen Activity): Train(9 subj × act12)  ->  Test(1 subj × act7)\n",
            "==========================================================================================\n",
            "[Fold 01] subject1_Frontal elevation of arms | Pred(win)=60.44 / GT=20.00 | MAE=40.44 | k_hat=1.34 | ent=0.411 | win_rate_mean=0.984\n",
            "[Fold 02] subject2_Frontal elevation of arms | Pred(win)=89.83 / GT=20.00 | MAE=69.83 | k_hat=1.13 | ent=0.247 | win_rate_mean=1.350\n",
            "[Fold 03] subject3_Frontal elevation of arms | Pred(win)=62.45 / GT=20.00 | MAE=42.45 | k_hat=2.43 | ent=0.620 | win_rate_mean=0.924\n",
            "[Fold 04] subject4_Frontal elevation of arms | Pred(win)=69.93 / GT=20.00 | MAE=49.93 | k_hat=3.32 | ent=1.078 | win_rate_mean=1.067\n",
            "[Fold 05] subject5_Frontal elevation of arms | Pred(win)=49.87 / GT=20.00 | MAE=29.87 | k_hat=1.48 | ent=0.555 | win_rate_mean=0.869\n",
            "[Fold 06] subject6_Frontal elevation of arms | Pred(win)=43.87 / GT=20.00 | MAE=23.87 | k_hat=1.52 | ent=0.630 | win_rate_mean=1.045\n",
            "[Fold 07] subject7_Frontal elevation of arms | Pred(win)=56.95 / GT=20.00 | MAE=36.95 | k_hat=3.11 | ent=0.822 | win_rate_mean=1.030\n",
            "[Fold 08] subject8_Frontal elevation of arms | Pred(win)=70.14 / GT=19.00 | MAE=51.14 | k_hat=2.39 | ent=0.761 | win_rate_mean=1.161\n",
            "[Fold 09] subject9_Frontal elevation of arms | Pred(win)=60.49 / GT=19.00 | MAE=41.49 | k_hat=1.16 | ent=0.286 | win_rate_mean=1.055\n",
            "[Fold 10] subject10_Frontal elevation of arms | Pred(win)=42.46 / GT=20.00 | MAE=22.46 | k_hat=1.21 | ent=0.330 | win_rate_mean=0.768\n",
            "\n",
            "==========================================================================================\n",
            " >>> A2 Final MAE mean: 40.842\n",
            " >>> A2 Final MAE std : 13.393\n",
            " >>> Num folds evaluated: 10\n",
            "==========================================================================================\n"
          ]
        }
      ]
    }
  ]
}