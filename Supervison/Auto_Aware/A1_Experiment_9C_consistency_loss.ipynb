{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 6,\n",
        "        \"TEST_ACT_ID\": 7,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 7: 20},\n",
        "            \"subject2\":  {6: 19, 7: 20},\n",
        "            \"subject3\":  {6: 21, 7: 20},\n",
        "            \"subject4\":  {6: 20, 7: 20},\n",
        "            \"subject5\":  {6: 20, 7: 20},\n",
        "            \"subject6\":  {6: 20, 7: 20},\n",
        "            \"subject7\":  {6: 20, 7: 20},\n",
        "            \"subject8\":  {6: 21, 7: 19},\n",
        "            \"subject9\":  {6: 21, 7: 19},\n",
        "            \"subject10\": {6: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h4IhNoNuihp",
        "outputId": "6088ff39-0458-4a95-ba70-7427e7694745"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=127\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act6  |  Test: all subjects x act7 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Frontal elevation of arms | Pred(win)=39.38 / GT=20.00 | MAE=19.38 | k_hat=1.96 | ent=0.728 | win_rate_mean=0.641\n",
            "[Test 02] subject2_Frontal elevation of arms | Pred(win)=26.87 / GT=20.00 | MAE=6.87 | k_hat=2.65 | ent=0.860 | win_rate_mean=0.404\n",
            "[Test 03] subject3_Frontal elevation of arms | Pred(win)=36.53 / GT=20.00 | MAE=16.53 | k_hat=2.16 | ent=0.789 | win_rate_mean=0.540\n",
            "[Test 04] subject4_Frontal elevation of arms | Pred(win)=37.74 / GT=20.00 | MAE=17.74 | k_hat=2.06 | ent=0.741 | win_rate_mean=0.576\n",
            "[Test 05] subject5_Frontal elevation of arms | Pred(win)=36.86 / GT=20.00 | MAE=16.86 | k_hat=1.96 | ent=0.712 | win_rate_mean=0.643\n",
            "[Test 06] subject6_Frontal elevation of arms | Pred(win)=29.31 / GT=20.00 | MAE=9.31 | k_hat=1.89 | ent=0.715 | win_rate_mean=0.698\n",
            "[Test 07] subject7_Frontal elevation of arms | Pred(win)=32.08 / GT=20.00 | MAE=12.08 | k_hat=2.02 | ent=0.746 | win_rate_mean=0.580\n",
            "[Test 08] subject8_Frontal elevation of arms | Pred(win)=26.67 / GT=19.00 | MAE=7.67 | k_hat=2.34 | ent=0.811 | win_rate_mean=0.441\n",
            "[Test 09] subject9_Frontal elevation of arms | Pred(win)=32.58 / GT=19.00 | MAE=13.58 | k_hat=2.06 | ent=0.764 | win_rate_mean=0.568\n",
            "[Test 10] subject10_Frontal elevation of arms | Pred(win)=31.98 / GT=20.00 | MAE=11.98 | k_hat=2.01 | ent=0.749 | win_rate_mean=0.578\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 13.199\n",
            " >>> A1 Final MAE std : 4.146\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            7: 'Frontal elevation of arms',\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 7,\n",
        "        \"TEST_ACT_ID\": 6,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 7: 20},\n",
        "            \"subject2\":  {6: 19, 7: 20},\n",
        "            \"subject3\":  {6: 21, 7: 20},\n",
        "            \"subject4\":  {6: 20, 7: 20},\n",
        "            \"subject5\":  {6: 20, 7: 20},\n",
        "            \"subject6\":  {6: 20, 7: 20},\n",
        "            \"subject7\":  {6: 20, 7: 20},\n",
        "            \"subject8\":  {6: 21, 7: 19},\n",
        "            \"subject9\":  {6: 21, 7: 19},\n",
        "            \"subject10\": {6: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VB1FQFaeOTJv",
        "outputId": "a9cf46dc-d5a8-4c3f-dbcf-34c374bd1b25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=132\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act7  |  Test: all subjects x act6 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Waist bends forward | Pred(win)=21.85 / GT=21.00 | MAE=0.85 | k_hat=1.44 | ent=0.504 | win_rate_mean=0.356\n",
            "[Test 02] subject2_Waist bends forward | Pred(win)=23.00 / GT=19.00 | MAE=4.00 | k_hat=1.38 | ent=0.469 | win_rate_mean=0.362\n",
            "[Test 03] subject3_Waist bends forward | Pred(win)=24.15 / GT=21.00 | MAE=3.15 | k_hat=1.43 | ent=0.495 | win_rate_mean=0.374\n",
            "[Test 04] subject4_Waist bends forward | Pred(win)=22.25 / GT=20.00 | MAE=2.25 | k_hat=1.52 | ent=0.549 | win_rate_mean=0.334\n",
            "[Test 05] subject5_Waist bends forward | Pred(win)=17.34 / GT=20.00 | MAE=2.66 | k_hat=1.82 | ent=0.704 | win_rate_mean=0.314\n",
            "[Test 06] subject6_Waist bends forward | Pred(win)=16.09 / GT=20.00 | MAE=3.91 | k_hat=1.63 | ent=0.618 | win_rate_mean=0.365\n",
            "[Test 07] subject7_Waist bends forward | Pred(win)=21.42 / GT=20.00 | MAE=1.42 | k_hat=1.41 | ent=0.499 | win_rate_mean=0.349\n",
            "[Test 08] subject8_Waist bends forward | Pred(win)=15.02 / GT=21.00 | MAE=5.98 | k_hat=1.80 | ent=0.604 | win_rate_mean=0.349\n",
            "[Test 09] subject9_Waist bends forward | Pred(win)=14.45 / GT=21.00 | MAE=6.55 | k_hat=1.74 | ent=0.790 | win_rate_mean=0.252\n",
            "[Test 10] subject10_Waist bends forward | Pred(win)=16.26 / GT=20.00 | MAE=3.74 | k_hat=1.59 | ent=0.512 | win_rate_mean=0.331\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 3.451\n",
            " >>> A1 Final MAE std : 1.722\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 6,\n",
        "        \"TEST_ACT_ID\": 8,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 8: 20},\n",
        "            \"subject2\":  {6: 19, 8: 21},\n",
        "            \"subject3\":  {6: 21, 8: 21},\n",
        "            \"subject4\":  {6: 20, 8: 19},\n",
        "            \"subject5\":  {6: 20, 8: 20},\n",
        "            \"subject6\":  {6: 20, 8: 20},\n",
        "            \"subject7\":  {6: 20, 8: 21},\n",
        "            \"subject8\":  {6: 21, 8: 21},\n",
        "            \"subject9\":  {6: 21, 8: 21},\n",
        "            \"subject10\": {6: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XY5Lx-rOv6d",
        "outputId": "3fff8333-874b-4ab6-d1a6-6bfe4e0edc55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=127\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act6  |  Test: all subjects x act8 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Knees bending | Pred(win)=41.27 / GT=20.00 | MAE=21.27 | k_hat=1.91 | ent=0.762 | win_rate_mean=0.611\n",
            "[Test 02] subject2_Knees bending | Pred(win)=39.64 / GT=21.00 | MAE=18.64 | k_hat=2.12 | ent=0.847 | win_rate_mean=0.578\n",
            "[Test 03] subject3_Knees bending | Pred(win)=23.01 / GT=21.00 | MAE=2.01 | k_hat=3.00 | ent=1.039 | win_rate_mean=0.362\n",
            "[Test 04] subject4_Knees bending | Pred(win)=17.42 / GT=19.00 | MAE=1.58 | k_hat=3.15 | ent=1.057 | win_rate_mean=0.279\n",
            "[Test 05] subject5_Knees bending | Pred(win)=20.05 / GT=20.00 | MAE=0.05 | k_hat=2.94 | ent=0.980 | win_rate_mean=0.369\n",
            "[Test 06] subject6_Knees bending | Pred(win)=16.68 / GT=20.00 | MAE=3.32 | k_hat=2.91 | ent=1.001 | win_rate_mean=0.362\n",
            "[Test 07] subject7_Knees bending | Pred(win)=22.08 / GT=21.00 | MAE=1.08 | k_hat=2.72 | ent=0.926 | win_rate_mean=0.392\n",
            "[Test 08] subject8_Knees bending | Pred(win)=17.06 / GT=21.00 | MAE=3.94 | k_hat=3.24 | ent=0.958 | win_rate_mean=0.333\n",
            "[Test 09] subject9_Knees bending | Pred(win)=28.60 / GT=21.00 | MAE=7.60 | k_hat=2.51 | ent=0.832 | win_rate_mean=0.482\n",
            "[Test 10] subject10_Knees bending | Pred(win)=28.24 / GT=21.00 | MAE=7.24 | k_hat=2.53 | ent=0.886 | win_rate_mean=0.493\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 6.673\n",
            " >>> A1 Final MAE std : 7.063\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 8,\n",
        "        \"TEST_ACT_ID\": 6,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {6: 21, 8: 20},\n",
        "            \"subject2\":  {6: 19, 8: 21},\n",
        "            \"subject3\":  {6: 21, 8: 21},\n",
        "            \"subject4\":  {6: 20, 8: 19},\n",
        "            \"subject5\":  {6: 20, 8: 20},\n",
        "            \"subject6\":  {6: 20, 8: 20},\n",
        "            \"subject7\":  {6: 20, 8: 21},\n",
        "            \"subject8\":  {6: 21, 8: 21},\n",
        "            \"subject9\":  {6: 21, 8: 21},\n",
        "            \"subject10\": {6: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ThXIJmgPHjz",
        "outputId": "d5febb11-005e-4c27-84c2-3f92af00d540"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=131\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act8  |  Test: all subjects x act6 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Waist bends forward | Pred(win)=50.75 / GT=21.00 | MAE=29.75 | k_hat=1.73 | ent=0.668 | win_rate_mean=0.826\n",
            "[Test 02] subject2_Waist bends forward | Pred(win)=30.40 / GT=19.00 | MAE=11.40 | k_hat=2.36 | ent=0.758 | win_rate_mean=0.479\n",
            "[Test 03] subject3_Waist bends forward | Pred(win)=41.26 / GT=21.00 | MAE=20.26 | k_hat=2.00 | ent=0.729 | win_rate_mean=0.639\n",
            "[Test 04] subject4_Waist bends forward | Pred(win)=48.15 / GT=20.00 | MAE=28.15 | k_hat=2.01 | ent=0.693 | win_rate_mean=0.723\n",
            "[Test 05] subject5_Waist bends forward | Pred(win)=26.95 / GT=20.00 | MAE=6.95 | k_hat=2.43 | ent=0.889 | win_rate_mean=0.487\n",
            "[Test 06] subject6_Waist bends forward | Pred(win)=32.40 / GT=20.00 | MAE=12.40 | k_hat=1.88 | ent=0.680 | win_rate_mean=0.736\n",
            "[Test 07] subject7_Waist bends forward | Pred(win)=41.74 / GT=20.00 | MAE=21.74 | k_hat=1.95 | ent=0.710 | win_rate_mean=0.679\n",
            "[Test 08] subject8_Waist bends forward | Pred(win)=24.60 / GT=21.00 | MAE=3.60 | k_hat=2.27 | ent=0.823 | win_rate_mean=0.572\n",
            "[Test 09] subject9_Waist bends forward | Pred(win)=37.35 / GT=21.00 | MAE=16.35 | k_hat=2.03 | ent=0.889 | win_rate_mean=0.651\n",
            "[Test 10] subject10_Waist bends forward | Pred(win)=30.24 / GT=20.00 | MAE=10.24 | k_hat=2.20 | ent=0.681 | win_rate_mean=0.615\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 16.083\n",
            " >>> A1 Final MAE std : 8.311\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 7,\n",
        "        \"TEST_ACT_ID\": 8,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {7: 20, 8: 20},\n",
        "            \"subject2\":  {7: 20, 8: 21},\n",
        "            \"subject3\":  {7: 20, 8: 21},\n",
        "            \"subject4\":  {7: 20, 8: 19},\n",
        "            \"subject5\":  {7: 20, 8: 20},\n",
        "            \"subject6\":  {7: 20, 8: 20},\n",
        "            \"subject7\":  {7: 20, 8: 21},\n",
        "            \"subject8\":  {7: 19, 8: 21},\n",
        "            \"subject9\":  {7: 19, 8: 21},\n",
        "            \"subject10\": {7: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB6_MravPKxd",
        "outputId": "a4365ca5-d5b5-480b-a920-19754aca3291"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=132\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act7  |  Test: all subjects x act8 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Knees bending | Pred(win)=22.03 / GT=20.00 | MAE=2.03 | k_hat=1.55 | ent=0.595 | win_rate_mean=0.326\n",
            "[Test 02] subject2_Knees bending | Pred(win)=18.82 / GT=21.00 | MAE=2.18 | k_hat=1.58 | ent=0.605 | win_rate_mean=0.274\n",
            "[Test 03] subject3_Knees bending | Pred(win)=18.17 / GT=21.00 | MAE=2.83 | k_hat=1.95 | ent=0.755 | win_rate_mean=0.286\n",
            "[Test 04] subject4_Knees bending | Pred(win)=17.12 / GT=19.00 | MAE=1.88 | k_hat=1.78 | ent=0.666 | win_rate_mean=0.274\n",
            "[Test 05] subject5_Knees bending | Pred(win)=12.37 / GT=20.00 | MAE=7.63 | k_hat=2.07 | ent=0.767 | win_rate_mean=0.228\n",
            "[Test 06] subject6_Knees bending | Pred(win)=12.11 / GT=20.00 | MAE=7.89 | k_hat=1.80 | ent=0.692 | win_rate_mean=0.263\n",
            "[Test 07] subject7_Knees bending | Pred(win)=14.08 / GT=21.00 | MAE=6.92 | k_hat=1.78 | ent=0.685 | win_rate_mean=0.250\n",
            "[Test 08] subject8_Knees bending | Pred(win)=12.00 / GT=21.00 | MAE=9.00 | k_hat=1.73 | ent=0.703 | win_rate_mean=0.234\n",
            "[Test 09] subject9_Knees bending | Pred(win)=13.45 / GT=21.00 | MAE=7.55 | k_hat=1.92 | ent=0.622 | win_rate_mean=0.226\n",
            "[Test 10] subject10_Knees bending | Pred(win)=11.87 / GT=21.00 | MAE=9.13 | k_hat=1.95 | ent=0.673 | win_rate_mean=0.207\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 5.705\n",
            " >>> A1 Final MAE std : 2.911\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            8: 'Knees bending',\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "            7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "            8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 8,\n",
        "        \"TEST_ACT_ID\": 7,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {7: 20, 8: 20},\n",
        "            \"subject2\":  {7: 20, 8: 21},\n",
        "            \"subject3\":  {7: 20, 8: 21},\n",
        "            \"subject4\":  {7: 20, 8: 19},\n",
        "            \"subject5\":  {7: 20, 8: 20},\n",
        "            \"subject6\":  {7: 20, 8: 20},\n",
        "            \"subject7\":  {7: 20, 8: 21},\n",
        "            \"subject8\":  {7: 19, 8: 21},\n",
        "            \"subject9\":  {7: 19, 8: 21},\n",
        "            \"subject10\": {7: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcYG8zQ2PnUj",
        "outputId": "503e15dc-e677-4920-ea59-ce5d18e4c3c3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=131\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act8  |  Test: all subjects x act7 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Frontal elevation of arms | Pred(win)=46.19 / GT=20.00 | MAE=26.19 | k_hat=1.82 | ent=0.631 | win_rate_mean=0.752\n",
            "[Test 02] subject2_Frontal elevation of arms | Pred(win)=33.54 / GT=20.00 | MAE=13.54 | k_hat=2.34 | ent=0.636 | win_rate_mean=0.504\n",
            "[Test 03] subject3_Frontal elevation of arms | Pred(win)=51.67 / GT=20.00 | MAE=31.67 | k_hat=1.93 | ent=0.586 | win_rate_mean=0.765\n",
            "[Test 04] subject4_Frontal elevation of arms | Pred(win)=53.56 / GT=20.00 | MAE=33.56 | k_hat=1.86 | ent=0.580 | win_rate_mean=0.817\n",
            "[Test 05] subject5_Frontal elevation of arms | Pred(win)=46.92 / GT=20.00 | MAE=26.92 | k_hat=1.76 | ent=0.609 | win_rate_mean=0.818\n",
            "[Test 06] subject6_Frontal elevation of arms | Pred(win)=41.20 / GT=20.00 | MAE=21.20 | k_hat=1.72 | ent=0.563 | win_rate_mean=0.982\n",
            "[Test 07] subject7_Frontal elevation of arms | Pred(win)=37.93 / GT=20.00 | MAE=17.93 | k_hat=1.86 | ent=0.641 | win_rate_mean=0.686\n",
            "[Test 08] subject8_Frontal elevation of arms | Pred(win)=31.54 / GT=19.00 | MAE=12.54 | k_hat=2.15 | ent=0.714 | win_rate_mean=0.522\n",
            "[Test 09] subject9_Frontal elevation of arms | Pred(win)=45.76 / GT=19.00 | MAE=26.76 | k_hat=1.80 | ent=0.592 | win_rate_mean=0.798\n",
            "[Test 10] subject10_Frontal elevation of arms | Pred(win)=42.50 / GT=20.00 | MAE=22.50 | k_hat=1.81 | ent=0.625 | win_rate_mean=0.769\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 23.282\n",
            " >>> A1 Final MAE std : 6.734\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 8,\n",
        "        \"TEST_ACT_ID\": 12,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 8: 20},\n",
        "            \"subject2\":  {12: 22, 8: 21},\n",
        "            \"subject3\":  {12: 21, 8: 21},\n",
        "            \"subject4\":  {12: 21, 8: 19},\n",
        "            \"subject5\":  {12: 20, 8: 20},\n",
        "            \"subject6\":  {12: 21, 8: 20},\n",
        "            \"subject7\":  {12: 19, 8: 21},\n",
        "            \"subject8\":  {12: 20, 8: 21},\n",
        "            \"subject9\":  {12: 20, 8: 21},\n",
        "            \"subject10\": {12: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFjwBq4MPvV5",
        "outputId": "02402c2e-adc0-4b88-9a72-ba06b7d4362a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=131\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act8  |  Test: all subjects x act12 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Jump front & back | Pred(win)=23.14 / GT=20.00 | MAE=3.14 | k_hat=1.72 | ent=0.627 | win_rate_mean=1.076\n",
            "[Test 02] subject2_Jump front & back | Pred(win)=24.20 / GT=22.00 | MAE=2.20 | k_hat=1.51 | ent=0.542 | win_rate_mean=1.182\n",
            "[Test 03] subject3_Jump front & back | Pred(win)=27.83 / GT=21.00 | MAE=6.83 | k_hat=1.51 | ent=0.499 | win_rate_mean=1.359\n",
            "[Test 04] subject4_Jump front & back | Pred(win)=28.02 / GT=21.00 | MAE=7.02 | k_hat=1.47 | ent=0.474 | win_rate_mean=1.368\n",
            "[Test 05] subject5_Jump front & back | Pred(win)=22.33 / GT=20.00 | MAE=2.33 | k_hat=1.59 | ent=0.643 | win_rate_mean=1.090\n",
            "[Test 06] subject6_Jump front & back | Pred(win)=29.00 / GT=21.00 | MAE=8.00 | k_hat=1.39 | ent=0.441 | win_rate_mean=1.416\n",
            "[Test 07] subject7_Jump front & back | Pred(win)=28.89 / GT=19.00 | MAE=9.89 | k_hat=1.45 | ent=0.520 | win_rate_mean=1.411\n",
            "[Test 08] subject8_Jump front & back | Pred(win)=20.45 / GT=20.00 | MAE=0.45 | k_hat=1.76 | ent=0.621 | win_rate_mean=0.999\n",
            "[Test 09] subject9_Jump front & back | Pred(win)=35.46 / GT=20.00 | MAE=15.46 | k_hat=1.36 | ent=0.471 | win_rate_mean=1.649\n",
            "[Test 10] subject10_Jump front & back | Pred(win)=31.14 / GT=20.00 | MAE=11.14 | k_hat=1.40 | ent=0.482 | win_rate_mean=1.521\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 6.647\n",
            " >>> A1 Final MAE std : 4.467\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            8: 'Knees bending',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             8: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 12,\n",
        "        \"TEST_ACT_ID\": 8,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 8: 20},\n",
        "            \"subject2\":  {12: 22, 8: 21},\n",
        "            \"subject3\":  {12: 21, 8: 21},\n",
        "            \"subject4\":  {12: 21, 8: 19},\n",
        "            \"subject5\":  {12: 20, 8: 20},\n",
        "            \"subject6\":  {12: 21, 8: 20},\n",
        "            \"subject7\":  {12: 19, 8: 21},\n",
        "            \"subject8\":  {12: 20, 8: 21},\n",
        "            \"subject9\":  {12: 20, 8: 21},\n",
        "            \"subject10\": {12: 20, 8: 21},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZHG2eM-QAap",
        "outputId": "2fc1e362-3e50-48b4-f9ce-c4c332b0cb56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=40\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act12  |  Test: all subjects x act8 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Knees bending | Pred(win)=73.08 / GT=20.00 | MAE=53.08 | k_hat=2.63 | ent=0.908 | win_rate_mean=1.081\n",
            "[Test 02] subject2_Knees bending | Pred(win)=60.46 / GT=21.00 | MAE=39.46 | k_hat=2.67 | ent=0.976 | win_rate_mean=0.881\n",
            "[Test 03] subject3_Knees bending | Pred(win)=60.96 / GT=21.00 | MAE=39.96 | k_hat=2.89 | ent=0.989 | win_rate_mean=0.960\n",
            "[Test 04] subject4_Knees bending | Pred(win)=56.52 / GT=19.00 | MAE=37.52 | k_hat=2.96 | ent=0.976 | win_rate_mean=0.905\n",
            "[Test 05] subject5_Knees bending | Pred(win)=45.62 / GT=20.00 | MAE=25.62 | k_hat=3.33 | ent=1.004 | win_rate_mean=0.840\n",
            "[Test 06] subject6_Knees bending | Pred(win)=41.48 / GT=20.00 | MAE=21.48 | k_hat=3.15 | ent=0.972 | win_rate_mean=0.900\n",
            "[Test 07] subject7_Knees bending | Pred(win)=49.95 / GT=21.00 | MAE=28.95 | k_hat=3.25 | ent=0.968 | win_rate_mean=0.887\n",
            "[Test 08] subject8_Knees bending | Pred(win)=43.39 / GT=21.00 | MAE=22.39 | k_hat=3.15 | ent=1.010 | win_rate_mean=0.848\n",
            "[Test 09] subject9_Knees bending | Pred(win)=52.73 / GT=21.00 | MAE=31.73 | k_hat=3.22 | ent=0.910 | win_rate_mean=0.888\n",
            "[Test 10] subject10_Knees bending | Pred(win)=47.50 / GT=21.00 | MAE=26.50 | k_hat=3.41 | ent=0.969 | win_rate_mean=0.828\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 32.668\n",
            " >>> A1 Final MAE std : 9.340\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 6,\n",
        "        \"TEST_ACT_ID\": 12,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 6: 21},\n",
        "            \"subject2\":  {12: 22, 6: 19},\n",
        "            \"subject3\":  {12: 21, 6: 21},\n",
        "            \"subject4\":  {12: 21, 6: 20},\n",
        "            \"subject5\":  {12: 20, 6: 20},\n",
        "            \"subject6\":  {12: 21, 6: 20},\n",
        "            \"subject7\":  {12: 19, 6: 20},\n",
        "            \"subject8\":  {12: 20, 6: 21},\n",
        "            \"subject9\":  {12: 20, 6: 21},\n",
        "            \"subject10\": {12: 20, 6: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT5jxPt4QHWW",
        "outputId": "9c7246f2-4e32-4831-f461-ccf104081c00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=127\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act6  |  Test: all subjects x act12 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Jump front & back | Pred(win)=12.31 / GT=20.00 | MAE=7.69 | k_hat=2.11 | ent=0.830 | win_rate_mean=0.573\n",
            "[Test 02] subject2_Jump front & back | Pred(win)=14.96 / GT=22.00 | MAE=7.04 | k_hat=1.79 | ent=0.720 | win_rate_mean=0.730\n",
            "[Test 03] subject3_Jump front & back | Pred(win)=17.13 / GT=21.00 | MAE=3.87 | k_hat=1.68 | ent=0.665 | win_rate_mean=0.837\n",
            "[Test 04] subject4_Jump front & back | Pred(win)=16.84 / GT=21.00 | MAE=4.16 | k_hat=1.64 | ent=0.647 | win_rate_mean=0.822\n",
            "[Test 05] subject5_Jump front & back | Pred(win)=13.98 / GT=20.00 | MAE=6.02 | k_hat=1.90 | ent=0.782 | win_rate_mean=0.683\n",
            "[Test 06] subject6_Jump front & back | Pred(win)=19.17 / GT=21.00 | MAE=1.83 | k_hat=1.60 | ent=0.593 | win_rate_mean=0.936\n",
            "[Test 07] subject7_Jump front & back | Pred(win)=14.78 / GT=19.00 | MAE=4.22 | k_hat=1.81 | ent=0.734 | win_rate_mean=0.722\n",
            "[Test 08] subject8_Jump front & back | Pred(win)=19.32 / GT=20.00 | MAE=0.68 | k_hat=1.80 | ent=0.709 | win_rate_mean=0.943\n",
            "[Test 09] subject9_Jump front & back | Pred(win)=17.96 / GT=20.00 | MAE=2.04 | k_hat=1.75 | ent=0.695 | win_rate_mean=0.835\n",
            "[Test 10] subject10_Jump front & back | Pred(win)=15.75 / GT=20.00 | MAE=4.25 | k_hat=1.83 | ent=0.743 | win_rate_mean=0.769\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 4.179\n",
            " >>> A1 Final MAE std : 2.152\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            6: 'Waist bends forward',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 12,\n",
        "        \"TEST_ACT_ID\": 6,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 6: 21},\n",
        "            \"subject2\":  {12: 22, 6: 19},\n",
        "            \"subject3\":  {12: 21, 6: 21},\n",
        "            \"subject4\":  {12: 21, 6: 20},\n",
        "            \"subject5\":  {12: 20, 6: 20},\n",
        "            \"subject6\":  {12: 21, 6: 20},\n",
        "            \"subject7\":  {12: 19, 6: 20},\n",
        "            \"subject8\":  {12: 20, 6: 21},\n",
        "            \"subject9\":  {12: 20, 6: 21},\n",
        "            \"subject10\": {12: 20, 6: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY-rBh30QVYU",
        "outputId": "e3dc37e3-0779-4f1f-978f-153b5de65379"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=40\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act12  |  Test: all subjects x act6 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Waist bends forward | Pred(win)=100.28 / GT=21.00 | MAE=79.28 | k_hat=1.79 | ent=0.644 | win_rate_mean=1.632\n",
            "[Test 02] subject2_Waist bends forward | Pred(win)=56.16 / GT=19.00 | MAE=37.16 | k_hat=2.48 | ent=0.895 | win_rate_mean=0.885\n",
            "[Test 03] subject3_Waist bends forward | Pred(win)=65.94 / GT=21.00 | MAE=44.94 | k_hat=2.32 | ent=0.829 | win_rate_mean=1.022\n",
            "[Test 04] subject4_Waist bends forward | Pred(win)=74.95 / GT=20.00 | MAE=54.95 | k_hat=2.33 | ent=0.861 | win_rate_mean=1.126\n",
            "[Test 05] subject5_Waist bends forward | Pred(win)=60.89 / GT=20.00 | MAE=40.89 | k_hat=2.46 | ent=0.890 | win_rate_mean=1.101\n",
            "[Test 06] subject6_Waist bends forward | Pred(win)=54.33 / GT=20.00 | MAE=34.33 | k_hat=2.31 | ent=0.906 | win_rate_mean=1.234\n",
            "[Test 07] subject7_Waist bends forward | Pred(win)=67.99 / GT=20.00 | MAE=47.99 | k_hat=2.39 | ent=0.906 | win_rate_mean=1.107\n",
            "[Test 08] subject8_Waist bends forward | Pred(win)=53.53 / GT=21.00 | MAE=32.53 | k_hat=2.31 | ent=0.835 | win_rate_mean=1.244\n",
            "[Test 09] subject9_Waist bends forward | Pred(win)=104.22 / GT=21.00 | MAE=83.22 | k_hat=1.71 | ent=0.739 | win_rate_mean=1.818\n",
            "[Test 10] subject10_Waist bends forward | Pred(win)=51.35 / GT=20.00 | MAE=31.35 | k_hat=2.70 | ent=0.866 | win_rate_mean=1.045\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 48.666\n",
            " >>> A1 Final MAE std : 17.730\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 7,\n",
        "        \"TEST_ACT_ID\": 12,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 7: 20},\n",
        "            \"subject2\":  {12: 22, 7: 20},\n",
        "            \"subject3\":  {12: 21, 7: 20},\n",
        "            \"subject4\":  {12: 21, 7: 20},\n",
        "            \"subject5\":  {12: 20, 7: 20},\n",
        "            \"subject6\":  {12: 21, 7: 20},\n",
        "            \"subject7\":  {12: 19, 7: 20},\n",
        "            \"subject8\":  {12: 20, 7: 19},\n",
        "            \"subject9\":  {12: 20, 7: 19},\n",
        "            \"subject10\": {12: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPQaTcLuQhDs",
        "outputId": "ff4bbbea-3e00-4219-de67-6c5b9d57e60c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=132\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act7  |  Test: all subjects x act12 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Jump front & back | Pred(win)=7.45 / GT=20.00 | MAE=12.55 | k_hat=1.93 | ent=0.743 | win_rate_mean=0.347\n",
            "[Test 02] subject2_Jump front & back | Pred(win)=8.88 / GT=22.00 | MAE=13.12 | k_hat=1.66 | ent=0.608 | win_rate_mean=0.434\n",
            "[Test 03] subject3_Jump front & back | Pred(win)=10.63 / GT=21.00 | MAE=10.37 | k_hat=1.45 | ent=0.513 | win_rate_mean=0.519\n",
            "[Test 04] subject4_Jump front & back | Pred(win)=9.70 / GT=21.00 | MAE=11.30 | k_hat=1.47 | ent=0.511 | win_rate_mean=0.474\n",
            "[Test 05] subject5_Jump front & back | Pred(win)=6.06 / GT=20.00 | MAE=13.94 | k_hat=2.02 | ent=0.801 | win_rate_mean=0.296\n",
            "[Test 06] subject6_Jump front & back | Pred(win)=7.94 / GT=21.00 | MAE=13.06 | k_hat=1.72 | ent=0.611 | win_rate_mean=0.388\n",
            "[Test 07] subject7_Jump front & back | Pred(win)=9.04 / GT=19.00 | MAE=9.96 | k_hat=1.67 | ent=0.651 | win_rate_mean=0.441\n",
            "[Test 08] subject8_Jump front & back | Pred(win)=7.60 / GT=20.00 | MAE=12.40 | k_hat=1.85 | ent=0.660 | win_rate_mean=0.371\n",
            "[Test 09] subject9_Jump front & back | Pred(win)=10.29 / GT=20.00 | MAE=9.71 | k_hat=1.67 | ent=0.630 | win_rate_mean=0.478\n",
            "[Test 10] subject10_Jump front & back | Pred(win)=10.69 / GT=20.00 | MAE=9.31 | k_hat=1.58 | ent=0.587 | win_rate_mean=0.522\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 11.573\n",
            " >>> A1 Final MAE std : 1.568\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n",
        "#\n",
        "# 핵심 아이디어\n",
        "# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n",
        "# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n",
        "# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n",
        "#\n",
        "# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n",
        "# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n",
        "# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n",
        "#\n",
        "# ✅ Added (ONLY): Time-warp consistency loss\n",
        "#   - 입력을 시간축으로 stretch/compress (time-warp)해도 \"count\"는 동일해야 한다는 제약\n",
        "#   - 모델 출력은 rate이므로, pred_count = rate_hat * duration 을 이용해 count 일관성을 강제\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Strict Seeding\n",
        "# ---------------------------------------------------------------------\n",
        "def set_strict_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Data Loading\n",
        "# ---------------------------------------------------------------------\n",
        "def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n",
        "    full_dataset = {}\n",
        "    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n",
        "\n",
        "    if not file_list:\n",
        "        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n",
        "        return {}\n",
        "\n",
        "    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n",
        "\n",
        "    for file_path in file_list:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        subj_part = file_name.split('.')[0]\n",
        "        try:\n",
        "            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n",
        "            subj_key = f\"subject{subj_id_num}\"\n",
        "        except:\n",
        "            subj_key = subj_part\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
        "            df = df.iloc[:, :len(column_names)]\n",
        "            df.columns = column_names\n",
        "\n",
        "            subj_data = {}\n",
        "            for label_code, activity_name in target_activities_map.items():\n",
        "                activity_df = df[df['activity_id'] == label_code].copy()\n",
        "                if not activity_df.empty:\n",
        "                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n",
        "\n",
        "            full_dataset[subj_key] = subj_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "            pass\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "\n",
        "def prepare_trial_list(label_config, full_data, target_map, feature_map):\n",
        "    trial_list = []\n",
        "    for subj, act_id, gt_count in label_config:\n",
        "        act_name = target_map.get(act_id)\n",
        "        feats = feature_map.get(act_id)\n",
        "\n",
        "        if subj in full_data and act_name in full_data[subj]:\n",
        "            raw_df = full_data[subj][act_name][feats]\n",
        "            raw_np = raw_df.values.astype(np.float32)\n",
        "\n",
        "            # Z-score 정규화 (표준화) 평균=0, std=1\n",
        "            mean = raw_np.mean(axis=0)\n",
        "            std = raw_np.std(axis=0) + 1e-6\n",
        "            norm_np = (raw_np - mean) / std\n",
        "\n",
        "            trial_list.append({\n",
        "                'data': norm_np,\n",
        "                'count': float(gt_count),\n",
        "                'meta': f\"{subj}_{act_name}\"\n",
        "            })\n",
        "        else:\n",
        "            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n",
        "\n",
        "    return trial_list\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2.5) Windowing (added)\n",
        "# ---------------------------------------------------------------------\n",
        "def trial_list_to_windows(trial_list, fs, win_sec=4.0, stride_sec=2.0, drop_last=True):\n",
        "    \"\"\"\n",
        "    trial_list: prepare_trial_list() 결과 (각 item에 'data'(T,C), 'count', 'meta')\n",
        "    window 라벨은 'trial-level rate'를 이용해서 window count를 만들기:\n",
        "      rate_trial = count_total / total_duration\n",
        "      count_window = rate_trial * window_duration\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    assert win_len > 0 and stride > 0\n",
        "\n",
        "    windows = []\n",
        "    for item in trial_list:\n",
        "        x = item[\"data\"]           # (T, C)\n",
        "        T = x.shape[0]\n",
        "        total_count = float(item[\"count\"])\n",
        "        meta = item[\"meta\"]\n",
        "\n",
        "        total_dur = max(T / float(fs), 1e-6)\n",
        "        rate_trial = total_count / total_dur  # reps/s (trial 전체 평균 rate)\n",
        "\n",
        "        if T < win_len:\n",
        "            # 너무 짧으면 그냥 한 개 윈도우로 취급 (pad 없이 variable length로)\n",
        "            win_dur = T / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x,\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[0:{T}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": 0,\n",
        "                \"win_end\": T,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        last_start = T - win_len\n",
        "        starts = list(range(0, last_start + 1, stride))\n",
        "\n",
        "        for st in starts:\n",
        "            ed = st + win_len\n",
        "            win_dur = win_len / float(fs)\n",
        "            windows.append({\n",
        "                \"data\": x[st:ed],\n",
        "                \"count\": rate_trial * win_dur,\n",
        "                \"meta\": f\"{meta}__win[{st}:{ed}]\",\n",
        "                \"parent_meta\": meta,\n",
        "                \"parent_T\": T,\n",
        "                \"win_start\": st,\n",
        "                \"win_end\": ed,\n",
        "            })\n",
        "\n",
        "        if not drop_last:\n",
        "            last_st = starts[-1] + stride\n",
        "            if last_st < T:\n",
        "                ed = T\n",
        "                win_dur = (ed - last_st) / float(fs)\n",
        "                windows.append({\n",
        "                    \"data\": x[last_st:ed],\n",
        "                    \"count\": rate_trial * win_dur,\n",
        "                    \"meta\": f\"{meta}__win[{last_st}:{ed}]\",\n",
        "                    \"parent_meta\": meta,\n",
        "                    \"parent_T\": T,\n",
        "                    \"win_start\": last_st,\n",
        "                    \"win_end\": ed,\n",
        "                })\n",
        "\n",
        "    return windows\n",
        "\n",
        "\n",
        "def predict_count_by_windowing(model, x_np, fs, win_sec, stride_sec, device, tau=1.0, batch_size=64):\n",
        "    \"\"\"\n",
        "    x_np: (T, C) normalized\n",
        "    return: pred_count (float), window_rates(np.ndarray)\n",
        "    \"\"\"\n",
        "    win_len = int(round(win_sec * fs))\n",
        "    stride = int(round(stride_sec * fs))\n",
        "    T = x_np.shape[0]\n",
        "    total_dur = T / float(fs)\n",
        "\n",
        "    if T <= win_len:\n",
        "        x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)  # (1,C,T)\n",
        "        with torch.no_grad():\n",
        "            rate_hat, _, _, _ = model(x_tensor, mask=None, tau=tau)\n",
        "        pred_count = float(rate_hat.item() * total_dur)\n",
        "        return pred_count, np.array([float(rate_hat.item())], dtype=np.float32)\n",
        "\n",
        "    starts = list(range(0, T - win_len + 1, stride))\n",
        "    windows = np.stack([x_np[st:st + win_len] for st in starts], axis=0)  # (N, win_len, C)\n",
        "\n",
        "    xw = torch.tensor(windows, dtype=torch.float32).permute(0, 2, 1).to(device)  # (N, C, win_len)\n",
        "\n",
        "    rates = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, xw.shape[0], batch_size):\n",
        "            xb = xw[i:i + batch_size]\n",
        "            r_hat, _, _, _ = model(xb, mask=None, tau=tau)  # (B,)\n",
        "            rates.append(r_hat.detach().cpu().numpy())\n",
        "\n",
        "    rates = np.concatenate(rates, axis=0)  # (N,)\n",
        "    rate_mean = float(rates.mean())\n",
        "    pred_count = rate_mean * total_dur\n",
        "    return float(pred_count), rates\n",
        "\n",
        "\n",
        "class TrialDataset(Dataset):\n",
        "    def __init__(self, trial_list):\n",
        "        self.trials = trial_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.trials[idx]\n",
        "        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n",
        "        count = torch.tensor(item['count'], dtype=torch.float32)\n",
        "        return data, count, item['meta']\n",
        "\n",
        "\n",
        "def collate_variable_length(batch):\n",
        "    max_len = max([x[0].shape[1] for x in batch])\n",
        "    C = batch[0][0].shape[0]\n",
        "\n",
        "    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n",
        "    for data, count, meta in batch:\n",
        "        T = data.shape[1]\n",
        "        lengths.append(T)\n",
        "\n",
        "        pad_size = max_len - T\n",
        "        if pad_size > 0:\n",
        "            pad = torch.zeros(C, pad_size)\n",
        "            d_padded = torch.cat([data, pad], dim=1)\n",
        "            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n",
        "        else:\n",
        "            d_padded = data\n",
        "            mask = torch.ones(T)\n",
        "\n",
        "        padded_data.append(d_padded)\n",
        "        masks.append(mask)\n",
        "        counts.append(count)\n",
        "        metas.append(meta)\n",
        "\n",
        "    return {\n",
        "        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n",
        "        \"mask\": torch.stack(masks),               # (B, T_max)\n",
        "        \"count\": torch.stack(counts),             # (B,)\n",
        "        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n",
        "        \"meta\": metas\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Model\n",
        "# ---------------------------------------------------------------------\n",
        "class ManifoldEncoder(nn.Module):\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, latent_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        z = self.net(x)            # (B, D, T)\n",
        "        z = z.transpose(1, 2)      # (B, T, D)\n",
        "        return z\n",
        "\n",
        "\n",
        "class ManifoldDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_dim, out_ch, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # z: (B, T, D)\n",
        "        zt = z.transpose(1, 2)     # (B, D, T)\n",
        "        x_hat = self.net(zt)       # (B, C, T)\n",
        "        return x_hat\n",
        "\n",
        "\n",
        "class MultiRateHead(nn.Module):\n",
        "    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n",
        "        super().__init__()\n",
        "        self.K_max = K_max\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, tau=1.0):\n",
        "        # z: (B,T,D)\n",
        "        out = self.net(z)                     # (B,T,1+K)\n",
        "        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n",
        "        phase_logits = out[..., 1:]           # (B,T,K)\n",
        "        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n",
        "        return amp, phase, phase_logits\n",
        "\n",
        "\n",
        "class KAutoCountModel(nn.Module):\n",
        "    \"\"\"\n",
        "    - outputs K_max micro-event rates r_k(t)\n",
        "    - predicts k_hat (>=1) per sample\n",
        "    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n",
        "        super().__init__()\n",
        "        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n",
        "        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n",
        "        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            b = self.rate_head.net[-1].bias\n",
        "            b.zero_()\n",
        "            b[0].fill_(-2.0)  # amp logit bias만 -2\n",
        "\n",
        "    @staticmethod\n",
        "    def _masked_mean_time(x, mask=None, eps=1e-6):\n",
        "        # x: (B,T) or (B,T,K)\n",
        "        if mask is None:\n",
        "            return x.mean(dim=1)\n",
        "        if x.dim() == 2:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n",
        "        elif x.dim() == 3:\n",
        "            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n",
        "            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n",
        "\n",
        "    def forward(self, x, mask=None, tau=1.0):\n",
        "        \"\"\"\n",
        "        x: (B,C,T), mask: (B,T)\n",
        "        return:\n",
        "          avg_rep_rate: (B,)\n",
        "        \"\"\"\n",
        "        z = self.encoder(x)              # (B,T,D)\n",
        "        x_hat = self.decoder(z)          # (B,C,T)\n",
        "\n",
        "        amp_t, phase_p, phase_logits = self.rate_head(z, tau=tau)\n",
        "        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n",
        "\n",
        "        # micro-event sum\n",
        "        micro_rate_t = amp_t\n",
        "\n",
        "        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n",
        "        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n",
        "\n",
        "        # rep rate\n",
        "        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n",
        "        if mask is not None:\n",
        "            rep_rate_t = rep_rate_t * mask\n",
        "\n",
        "        # avg rep rate (masked mean)\n",
        "        if mask is None:\n",
        "            avg_rep_rate = rep_rate_t.mean(dim=1)\n",
        "        else:\n",
        "            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n",
        "\n",
        "        aux = {\n",
        "            \"rates_k_t\": rates_k_t,          # (B,T,K)\n",
        "            \"phase_p\": phase_p,              # (B,T,K)\n",
        "            \"phase_logits\": phase_logits,    # (B,T,K)\n",
        "            \"micro_rate_t\": micro_rate_t,    # (B,T)\n",
        "            \"rep_rate_t\": rep_rate_t,        # (B,T)\n",
        "            \"k_hat\": k_hat,                  # (B,)\n",
        "        }\n",
        "        return avg_rep_rate, z, x_hat, aux\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Loss utils\n",
        "# ---------------------------------------------------------------------\n",
        "def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n",
        "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
        "    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n",
        "    se = (x_hat - x) ** 2                    # (B,C,T)\n",
        "    se = se * mask_bc\n",
        "    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n",
        "    return se.sum() / denom\n",
        "\n",
        "\n",
        "def temporal_smoothness(v, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    v: (B,T) -> L1 smoothness on first difference\n",
        "    \"\"\"\n",
        "    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n",
        "    if mask is None:\n",
        "        return dv.mean()\n",
        "    m = mask[:, 1:] * mask[:, :-1]\n",
        "    m = m.to(dtype=dv.dtype, device=dv.device)\n",
        "    return (dv * m).sum() / (m.sum() + eps)\n",
        "\n",
        "\n",
        "def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n",
        "    \"\"\"\n",
        "    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n",
        "    phase_p: (B,T,K)\n",
        "    \"\"\"\n",
        "    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n",
        "    if mask is None:\n",
        "        return ent.mean()\n",
        "    ent = ent * mask\n",
        "    return ent.sum() / (mask.sum() + eps)\n",
        "\n",
        "\n",
        "def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n",
        "    \"\"\"\n",
        "    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n",
        "    effK = 1 / sum(p_bar^2)  in [1,K]\n",
        "    \"\"\"\n",
        "    if mask is None:\n",
        "        p_bar = phase_p.mean(dim=1)  # (B,K)\n",
        "    else:\n",
        "        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n",
        "        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n",
        "\n",
        "    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n",
        "    return effK.mean(), effK.detach()\n",
        "\n",
        "\n",
        "def time_warp_batch(x, mask, length, warp_min=0.7, warp_max=1.3, min_len=8):\n",
        "    \"\"\"\n",
        "    x: (B,C,Tmax) padded\n",
        "    mask: (B,Tmax)\n",
        "    length: (B,) float or int, valid length\n",
        "    return:\n",
        "      xw: (B,C,Tw_max) padded time-warped\n",
        "      mw: (B,Tw_max)\n",
        "      lw: (B,) float (warped valid length)\n",
        "    \"\"\"\n",
        "    B, C, Tmax = x.shape\n",
        "    device = x.device\n",
        "    dtype = x.dtype\n",
        "\n",
        "    xw_list, mw_list, lw_list = [], [], []\n",
        "    Tw_max = 0\n",
        "\n",
        "    # per-sample warp (tempo 변화)\n",
        "    for b in range(B):\n",
        "        Tb = int(length[b].item())\n",
        "        Tb = max(Tb, 1)\n",
        "\n",
        "        xb = x[b:b+1, :, :Tb]  # (1,C,Tb)\n",
        "\n",
        "        # warp factor\n",
        "        w = float(torch.empty(1, device=device).uniform_(warp_min, warp_max).item())\n",
        "        Tw = int(round(Tb * w))\n",
        "        Tw = max(Tw, min_len)\n",
        "\n",
        "        # resample to Tw (linear)\n",
        "        xbw = F.interpolate(xb, size=Tw, mode=\"linear\", align_corners=False)  # (1,C,Tw)\n",
        "\n",
        "        mw = torch.ones((Tw,), device=device, dtype=mask.dtype)\n",
        "\n",
        "        xw_list.append(xbw.squeeze(0))  # (C,Tw)\n",
        "        mw_list.append(mw)              # (Tw,)\n",
        "        lw_list.append(float(Tw))\n",
        "\n",
        "        if Tw > Tw_max:\n",
        "            Tw_max = Tw\n",
        "\n",
        "    # pad to Tw_max\n",
        "    xw = torch.zeros((B, C, Tw_max), device=device, dtype=dtype)\n",
        "    mw = torch.zeros((B, Tw_max), device=device, dtype=mask.dtype)\n",
        "    for b in range(B):\n",
        "        xbw = xw_list[b]\n",
        "        m_b = mw_list[b]\n",
        "        Tw = xbw.shape[1]\n",
        "        xw[b, :, :Tw] = xbw\n",
        "        mw[b, :Tw] = m_b\n",
        "\n",
        "    lw = torch.tensor(lw_list, device=device, dtype=length.dtype)\n",
        "    return xw, mw, lw\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Train  (ONLY MODIFIED PART: remove shift, add time-warp consistency)\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, optimizer, config, device):\n",
        "    model.train()\n",
        "    stats = {k: 0.0 for k in [\n",
        "        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk', 'loss_warp',\n",
        "        'mae_count'\n",
        "    ]}\n",
        "\n",
        "    fs = config[\"fs\"]\n",
        "    tau = config.get(\"tau\", 1.0)\n",
        "\n",
        "    lam_recon = config.get(\"lambda_recon\", 1.0)\n",
        "    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n",
        "    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n",
        "    lam_effk = config.get(\"lambda_effk\", 0.005)\n",
        "\n",
        "    # ✅ Added: time-warp consistency weight & range\n",
        "    lam_warp = config.get(\"lambda_warp\", 0.0)\n",
        "    warp_min = config.get(\"warp_min\", 0.7)\n",
        "    warp_max = config.get(\"warp_max\", 1.3)\n",
        "    warp_detach_ref = bool(config.get(\"warp_detach_ref\", True))\n",
        "\n",
        "    for batch in loader:\n",
        "        x = batch[\"data\"].to(device)         # (B,C,T)\n",
        "        mask = batch[\"mask\"].to(device)      # (B,T)\n",
        "        y_count = batch[\"count\"].to(device)  # (B,)\n",
        "        length = batch[\"length\"].to(device)  # (B,)\n",
        "\n",
        "        duration = torch.clamp(length / fs, min=1e-6)  # sec\n",
        "        y_rate = y_count / duration                    # reps/s\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n",
        "\n",
        "        # (1) 속도 맞추기 (MSE Loss)\n",
        "        loss_rate = F.mse_loss(rate_hat, y_rate)\n",
        "\n",
        "        # (2) recon\n",
        "        loss_recon = masked_recon_mse(x_hat, x, mask)\n",
        "\n",
        "        # (3) smoothness (rep_rate_t 기준)\n",
        "        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n",
        "\n",
        "        # (4) phase exclusivity (entropy)\n",
        "        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (5) effective-K usage (overall)\n",
        "        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n",
        "\n",
        "        # (6) ✅ Time-warp consistency loss (count-level)\n",
        "        #     pred_count = rate_hat * duration 이 time-warp 후에도 동일해야 함\n",
        "        if lam_warp > 0.0:\n",
        "            xw, mw, lw = time_warp_batch(\n",
        "                x=x, mask=mask, length=length,\n",
        "                warp_min=warp_min, warp_max=warp_max, min_len=8\n",
        "            )\n",
        "            dur_w = torch.clamp(lw / fs, min=1e-6)\n",
        "\n",
        "            rate_hat_w, _, _, _ = model(xw, mw, tau=tau)\n",
        "\n",
        "            pred_count = rate_hat * duration\n",
        "            pred_count_w = rate_hat_w * dur_w\n",
        "\n",
        "            if warp_detach_ref:\n",
        "                pred_count_ref = pred_count.detach()\n",
        "            else:\n",
        "                pred_count_ref = pred_count\n",
        "\n",
        "            loss_warp = F.l1_loss(pred_count_w, pred_count_ref)\n",
        "        else:\n",
        "            loss_warp = torch.zeros((), device=device)\n",
        "\n",
        "        loss = (loss_rate\n",
        "                + lam_recon * loss_recon\n",
        "                + lam_smooth * loss_smooth\n",
        "                + lam_phase_ent * loss_phase_ent\n",
        "                + lam_effk * loss_effk\n",
        "                + lam_warp * loss_warp)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # MAE on count\n",
        "        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n",
        "        stats['loss'] += loss.item()\n",
        "        stats['loss_rate'] += loss_rate.item()\n",
        "        stats['loss_recon'] += loss_recon.item()\n",
        "        stats['loss_smooth'] += loss_smooth.item()\n",
        "        stats['loss_phase_ent'] += loss_phase_ent.item()\n",
        "        stats['loss_effk'] += loss_effk.item()\n",
        "        stats['loss_warp'] += float(loss_warp.item())\n",
        "        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n",
        "\n",
        "    n = len(loader)\n",
        "    return {k: v / n for k, v in stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 6) Visualization helpers (subject-wise subplot)\n",
        "# ---------------------------------------------------------------------\n",
        "def _smooth_1d(y, sigma=2.0):\n",
        "    \"\"\"\n",
        "    지저분한 노이즈를 다듬어서 시각화\n",
        "    \"\"\"\n",
        "    y = np.asarray(y, dtype=np.float32)\n",
        "    return gaussian_filter1d(y, sigma=sigma)\n",
        "\n",
        "\n",
        "def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T,K) numpy\n",
        "    return: time-avg entropy (scalar)\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n",
        "    return float(ent_t.mean())\n",
        "\n",
        "\n",
        "def downsample_time_axis(arr, max_T=2000):\n",
        "    \"\"\"\n",
        "    arr: (T, ...) numpy\n",
        "    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n",
        "    return: arr_ds, idx (원래 시간 인덱스)\n",
        "    \"\"\"\n",
        "    T = arr.shape[0]\n",
        "    if T <= max_T:\n",
        "        idx = np.arange(T)\n",
        "        return arr, idx\n",
        "    idx = np.linspace(0, T - 1, max_T).astype(int)\n",
        "    return arr[idx], idx\n",
        "\n",
        "\n",
        "def plot_phase_heatmap_and_dominant(\n",
        "    phase_p_np,\n",
        "    fs,\n",
        "    title=\"phase_p heatmap + dominant phase\",\n",
        "    max_T=2000\n",
        "):\n",
        "    \"\"\"\n",
        "    phase_p_np: (T, K) numpy array\n",
        "    fs: sampling rate\n",
        "    max_T: 시각화 다운샘플링 길이\n",
        "    \"\"\"\n",
        "    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n",
        "    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n",
        "\n",
        "    # (1) downsample for visualization\n",
        "    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n",
        "    Tds, K = phase_ds.shape\n",
        "    t_sec = idx / float(fs)\n",
        "\n",
        "    # (2) dominant phase (argmax)\n",
        "    dom = np.argmax(phase_ds, axis=1)  # (T',)\n",
        "\n",
        "    # (3) plot\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n",
        "\n",
        "    # --- Heatmap (top) ---\n",
        "    ax0 = fig.add_subplot(gs[0, 0])\n",
        "    im = ax0.imshow(\n",
        "        phase_ds.T,                 # (K, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n",
        "    )\n",
        "    ax0.set_title(title, fontsize=24, pad=10)\n",
        "    ax0.set_ylabel(\"Phase k\", fontsize=18)\n",
        "    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n",
        "    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n",
        "\n",
        "    # --- Dominant phase timeline (bottom) ---\n",
        "    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n",
        "    ax1.imshow(\n",
        "        dom[None, :],               # (1, T')\n",
        "        aspect=\"auto\",\n",
        "        origin=\"lower\",\n",
        "        interpolation=\"nearest\",\n",
        "        extent=[t_sec[0], t_sec[-1], 0, 1]\n",
        "    )\n",
        "    ax1.set_yticks([])\n",
        "    ax1.set_ylabel(\"dominant\", fontsize=14)\n",
        "    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n",
        "    \"\"\"\n",
        "    viz_cache: list of dict\n",
        "      each dict contains:\n",
        "        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n",
        "    \"\"\"\n",
        "    if viz_cache is None or len(viz_cache) == 0:\n",
        "        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n",
        "        return\n",
        "\n",
        "    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n",
        "    colors = sns.color_palette(\"muted\")\n",
        "    c_rate = colors[0]\n",
        "    c_count = colors[1]\n",
        "\n",
        "    n = len(viz_cache)\n",
        "    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "    axes = np.array(axes).flatten()\n",
        "\n",
        "    fig.suptitle(title, fontsize=40, y=0.995)\n",
        "\n",
        "    for i, item in enumerate(viz_cache):\n",
        "        ax = axes[i]\n",
        "\n",
        "        t = item[\"t\"]\n",
        "        rep_rate = item[\"rep_rate\"]\n",
        "        gt_count = item[\"gt\"]\n",
        "        pred_count = item[\"pred\"]\n",
        "        diff = item[\"diff\"]\n",
        "        k_hat = item[\"k_hat\"]\n",
        "        entropy = item[\"entropy\"]\n",
        "        test_subj = item[\"test_subj\"]\n",
        "        fold = item[\"fold\"]\n",
        "\n",
        "        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n",
        "        cum = np.cumsum(rep_rate) / fs\n",
        "\n",
        "        # 왼쪽: rep_rate\n",
        "        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n",
        "        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n",
        "        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n",
        "        ax.grid(True, linestyle='--', alpha=0.5)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
        "\n",
        "        # 오른쪽: cumulative count\n",
        "        ax2 = ax.twinx()\n",
        "        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n",
        "        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n",
        "        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n",
        "        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n",
        "        ax2.grid(False)\n",
        "\n",
        "        ax.set_title(\n",
        "            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n",
        "            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n",
        "            fontsize=34, pad=10\n",
        "        )\n",
        "        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.985])\n",
        "    plt.subplots_adjust(hspace=0.5)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 7) Main (A1: Unseen Activity only, WITH windowing)\n",
        "# ---------------------------------------------------------------------\n",
        "def build_label_tuples_from_table(subjects, act_id, count_table):\n",
        "    \"\"\"\n",
        "    count_table: dict like { \"subject1\": {6:21, 7:20}, ... }\n",
        "    returns: list of (subj, act_id, gt_count)\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    for s in subjects:\n",
        "        if s not in count_table:\n",
        "            continue\n",
        "        if act_id not in count_table[s]:\n",
        "            continue\n",
        "        labels.append((s, act_id, float(count_table[s][act_id])))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def main():\n",
        "    CONFIG = {\n",
        "        \"seed\": 42,\n",
        "        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n",
        "\n",
        "        \"COLUMN_NAMES\": [\n",
        "            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "            'ecg_1', 'ecg_2',\n",
        "            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n",
        "            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n",
        "            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n",
        "            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n",
        "            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n",
        "            'activity_id'\n",
        "        ],\n",
        "        \"TARGET_ACTIVITIES_MAP\": {\n",
        "            7: 'Frontal elevation of arms',\n",
        "            12: 'Jump front & back'\n",
        "        },\n",
        "\n",
        "        # ✅ train/test 모두 C를 동일하게 유지해야 같은 모델로 평가 가능\n",
        "        \"ACT_FEATURE_MAP\": {\n",
        "             7: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z'],\n",
        "             12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n",
        "                'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n",
        "                'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n",
        "        },\n",
        "\n",
        "        # Training Params\n",
        "        \"epochs\": 100,\n",
        "        \"lr\": 5e-4,\n",
        "        \"batch_size\": 64,\n",
        "        \"fs\": 50,\n",
        "\n",
        "        # Model\n",
        "        \"hidden_dim\": 128,\n",
        "        \"latent_dim\": 16,\n",
        "        \"K_max\": 6,\n",
        "\n",
        "        # Loss Weights\n",
        "        \"lambda_recon\": 1.0,\n",
        "        \"lambda_smooth\": 0.05,\n",
        "        \"lambda_phase_ent\": 0.01,\n",
        "        \"lambda_effk\": 0.0005,\n",
        "\n",
        "        # ✅ Added: Time-warp consistency (ONLY NEW)\n",
        "        \"lambda_warp\": 0.2,      # 시작은 1e-2~5e-2 추천\n",
        "        \"warp_min\": 0.5,          # 느리게(늘리기)\n",
        "        \"warp_max\": 1.5,          # 빠르게(줄이기)\n",
        "        \"warp_detach_ref\": True,  # warped 쪽만 원본 count로 맞추게(안정)\n",
        "\n",
        "        # temperature (phase 경쟁 강도)\n",
        "        \"tau\": 1.0,\n",
        "\n",
        "        # A1 setting\n",
        "        \"TRAIN_ACT_ID\": 12,\n",
        "        \"TEST_ACT_ID\": 7,\n",
        "\n",
        "        # -------------------------\n",
        "        # ✅ Windowing (added)\n",
        "        # -------------------------\n",
        "        \"USE_WINDOWING\": True,\n",
        "        \"WIN_SEC\": 8.0,\n",
        "        \"STRIDE_SEC\": 4.0,\n",
        "        \"DROP_LAST\": True,\n",
        "\n",
        "        # ✅ dict 형태로 고쳐야 함\n",
        "        \"COUNT_TABLE\": {\n",
        "            \"subject1\":  {12: 20, 7: 20},\n",
        "            \"subject2\":  {12: 22, 7: 20},\n",
        "            \"subject3\":  {12: 21, 7: 20},\n",
        "            \"subject4\":  {12: 21, 7: 20},\n",
        "            \"subject5\":  {12: 20, 7: 20},\n",
        "            \"subject6\":  {12: 21, 7: 20},\n",
        "            \"subject7\":  {12: 19, 7: 20},\n",
        "            \"subject8\":  {12: 20, 7: 19},\n",
        "            \"subject9\":  {12: 20, 7: 19},\n",
        "            \"subject10\": {12: 20, 7: 20},\n",
        "        },\n",
        "    }\n",
        "\n",
        "    set_strict_seed(CONFIG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    full_data = load_mhealth_dataset(\n",
        "        CONFIG[\"data_dir\"],\n",
        "        CONFIG[\"TARGET_ACTIVITIES_MAP\"],\n",
        "        CONFIG[\"COLUMN_NAMES\"]\n",
        "    )\n",
        "    if not full_data:\n",
        "        return\n",
        "\n",
        "    subjects = [f\"subject{i}\" for i in range(1, 11)]\n",
        "\n",
        "    # -------------------------\n",
        "    # A1 split (NO LOSO)\n",
        "    # Train: all subjects x TRAIN_ACT_ID\n",
        "    # Test : all subjects x TEST_ACT_ID (unseen activity)\n",
        "    # -------------------------\n",
        "    train_labels = build_label_tuples_from_table(subjects, CONFIG[\"TRAIN_ACT_ID\"], CONFIG[\"COUNT_TABLE\"])\n",
        "    test_labels  = build_label_tuples_from_table(subjects, CONFIG[\"TEST_ACT_ID\"],  CONFIG[\"COUNT_TABLE\"])\n",
        "\n",
        "    if len(train_labels) == 0:\n",
        "        print(\"[Error] No train labels. Check COUNT_TABLE / TRAIN_ACT_ID.\")\n",
        "        return\n",
        "    if len(test_labels) == 0:\n",
        "        print(\"[Error] No test labels. Check COUNT_TABLE / TEST_ACT_ID.\")\n",
        "        return\n",
        "\n",
        "    train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "    test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n",
        "\n",
        "    if len(train_data) == 0:\n",
        "        print(\"[Error] train_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "    if len(test_data) == 0:\n",
        "        print(\"[Error] test_data empty. (missing files or ACT_FEATURE_MAP mismatch)\")\n",
        "        return\n",
        "\n",
        "    # -------------------------\n",
        "    # ✅ Windowing 적용 (Train에만)\n",
        "    # -------------------------\n",
        "    if CONFIG.get(\"USE_WINDOWING\", False):\n",
        "        train_windows = trial_list_to_windows(\n",
        "            train_data,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            drop_last=CONFIG[\"DROP_LAST\"],\n",
        "        )\n",
        "        print(f\"[Windowing] train trials={len(train_data)} -> train windows={len(train_windows)}\")\n",
        "        train_data_for_loader = train_windows\n",
        "    else:\n",
        "        train_data_for_loader = train_data\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(train_data_for_loader),\n",
        "        batch_size=CONFIG[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_variable_length,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    input_ch = train_data[0]['data'].shape[1]\n",
        "    model = KAutoCountModel(\n",
        "        input_ch=input_ch,\n",
        "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
        "        latent_dim=CONFIG[\"latent_dim\"],\n",
        "        K_max=CONFIG[\"K_max\"]\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\" >>> A1 Train: all subjects x act{CONFIG['TRAIN_ACT_ID']}  |  Test: all subjects x act{CONFIG['TEST_ACT_ID']} (unseen activity)\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # ---- Train ----\n",
        "    for epoch in range(CONFIG[\"epochs\"]):\n",
        "        _ = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n",
        "        scheduler.step()\n",
        "\n",
        "    # ---- Test (per-subject) ----\n",
        "    model.eval()\n",
        "    maes = []\n",
        "    viz_cache = []\n",
        "\n",
        "    for idx, item in enumerate(test_data):\n",
        "        x_np = item[\"data\"]  # (T,C)\n",
        "        gt_count = float(item[\"count\"])\n",
        "\n",
        "        # ✅ windowing으로 pred_count 계산 (Test)\n",
        "        pred_count, win_rates = predict_count_by_windowing(\n",
        "            model,\n",
        "            x_np=x_np,\n",
        "            fs=CONFIG[\"fs\"],\n",
        "            win_sec=CONFIG[\"WIN_SEC\"],\n",
        "            stride_sec=CONFIG[\"STRIDE_SEC\"],\n",
        "            device=device,\n",
        "            tau=CONFIG[\"tau\"],\n",
        "            batch_size=CONFIG[\"batch_size\"]\n",
        "        )\n",
        "\n",
        "        mae = abs(pred_count - gt_count)\n",
        "        maes.append(mae)\n",
        "\n",
        "        # 시각화용(원래처럼 full forward 1회)\n",
        "        with torch.no_grad():\n",
        "            x_tensor = torch.tensor(x_np, dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n",
        "            rate_hat_full, _, _, aux = model(x_tensor, mask=None, tau=CONFIG[\"tau\"])\n",
        "\n",
        "            phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()\n",
        "            k_hat = float(aux[\"k_hat\"].item())\n",
        "            ent = compute_phase_entropy_mean(phase_p)\n",
        "            rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()\n",
        "            T = rep_rate.shape[0]\n",
        "            t = np.arange(T) / float(CONFIG[\"fs\"])\n",
        "\n",
        "        viz_cache.append({\n",
        "            \"fold\": idx + 1,   # 그냥 인덱스\n",
        "            \"test_subj\": item[\"meta\"],  # subject+act name\n",
        "            \"t\": t,\n",
        "            \"rep_rate\": rep_rate,\n",
        "            \"gt\": gt_count,\n",
        "            \"pred\": float(pred_count),   # ✅ windowing pred\n",
        "            \"diff\": float(pred_count - gt_count),\n",
        "            \"k_hat\": k_hat,\n",
        "            \"entropy\": ent,\n",
        "            \"phase_p\": phase_p,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"[Test {idx+1:02d}] {item['meta']} | Pred(win)={pred_count:.2f} / GT={gt_count:.2f} | \"\n",
        "            f\"MAE={mae:.2f} | k_hat={k_hat:.2f} | ent={ent:.3f} | win_rate_mean={win_rates.mean():.3f}\"\n",
        "        )\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(f\" >>> A1 Final MAE mean: {np.mean(maes):.3f}\")\n",
        "    print(f\" >>> A1 Final MAE std : {np.std(maes):.3f}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # 시각화(원하면 유지)\n",
        "    # plot_folds_test_subplot(\n",
        "    #     viz_cache,\n",
        "    #     fs=CONFIG[\"fs\"],\n",
        "    #     title=f\"A1 TEST visualization | Train act{CONFIG['TRAIN_ACT_ID']} -> Test act{CONFIG['TEST_ACT_ID']}\"\n",
        "    # )\n",
        "\n",
        "    # for item in viz_cache:\n",
        "    #     plot_phase_heatmap_and_dominant(\n",
        "    #         item[\"phase_p\"],\n",
        "    #         fs=CONFIG[\"fs\"],\n",
        "    #         title=f\"{item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n",
        "    #         max_T=2000\n",
        "    #     )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FZPAmRBQqlT",
        "outputId": "3f57d4c6-8f95-43e5-bc07-c1dc62e9676f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading 10 subjects from /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET...\n",
            "[Windowing] train trials=10 -> train windows=40\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Train: all subjects x act12  |  Test: all subjects x act7 (unseen activity)\n",
            "--------------------------------------------------------------------------------\n",
            "[Test 01] subject1_Frontal elevation of arms | Pred(win)=58.06 / GT=20.00 | MAE=38.06 | k_hat=2.77 | ent=0.763 | win_rate_mean=0.945\n",
            "[Test 02] subject2_Frontal elevation of arms | Pred(win)=52.44 / GT=20.00 | MAE=32.44 | k_hat=3.59 | ent=0.812 | win_rate_mean=0.788\n",
            "[Test 03] subject3_Frontal elevation of arms | Pred(win)=66.63 / GT=20.00 | MAE=46.63 | k_hat=2.79 | ent=0.755 | win_rate_mean=0.986\n",
            "[Test 04] subject4_Frontal elevation of arms | Pred(win)=68.65 / GT=20.00 | MAE=48.65 | k_hat=2.64 | ent=0.756 | win_rate_mean=1.047\n",
            "[Test 05] subject5_Frontal elevation of arms | Pred(win)=57.71 / GT=20.00 | MAE=37.71 | k_hat=2.67 | ent=0.826 | win_rate_mean=1.006\n",
            "[Test 06] subject6_Frontal elevation of arms | Pred(win)=51.56 / GT=20.00 | MAE=31.56 | k_hat=2.46 | ent=0.727 | win_rate_mean=1.228\n",
            "[Test 07] subject7_Frontal elevation of arms | Pred(win)=48.46 / GT=20.00 | MAE=28.46 | k_hat=2.92 | ent=0.773 | win_rate_mean=0.876\n",
            "[Test 08] subject8_Frontal elevation of arms | Pred(win)=48.49 / GT=19.00 | MAE=29.49 | k_hat=3.02 | ent=0.858 | win_rate_mean=0.803\n",
            "[Test 09] subject9_Frontal elevation of arms | Pred(win)=54.13 / GT=19.00 | MAE=35.13 | k_hat=2.77 | ent=0.783 | win_rate_mean=0.944\n",
            "[Test 10] subject10_Frontal elevation of arms | Pred(win)=51.91 / GT=20.00 | MAE=31.91 | k_hat=2.72 | ent=0.763 | win_rate_mean=0.939\n",
            "--------------------------------------------------------------------------------\n",
            " >>> A1 Final MAE mean: 36.005\n",
            " >>> A1 Final MAE std : 6.552\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}
