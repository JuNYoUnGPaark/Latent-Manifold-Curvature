{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1IaZRmMUUSr2XoP14cSM9R6w2AIOUWavb","authorship_tag":"ABX9TyOQoGLFyu5E3r15APMnXy0N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Uzcl-2ADya4vcvdYGx_rVVZwbMzMa9qk"},"id":"t1wxP5BG7WWn","executionInfo":{"status":"ok","timestamp":1766724267135,"user_tz":-540,"elapsed":44660,"user":{"displayName":"박준영","userId":"08318675183777163575"}},"outputId":"a14416f5-b01e-4279-b2f7-22702ce36cdb"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# =========================\n","# Count-only K-auto (Multi-event) version  (NO manual Pair/lag/overlap/balance)\n","#\n","# 핵심 아이디어\n","# - Micro-event Rate 예측: 모델은 하나의 통합된 속도가 아니라, K_max개의 서로 다른 '작은 단위 동작'의 속도 흐름(r_k(t))을 예측\n","# - 샘플마다 \"rep당 micro-event 개수\" k_hat(>=1)을 스스로 추정\n","# - 우리가 주는 감독은 오직 rep count(=20) -> rep rate만 맞추게\n","#\n","# 1) rate head를 amp(t) * softmax(phase) 형태로 바꿔서 K-stream 간 \"경쟁\"이 생기게 함\n","# 2) k_hat을 따로 head로 예측하지 않고, phase 사용 분포로부터 effK(=effective K)로 정의 (자동 K)\n","# 3) phase sparsity/exclusivity를 위해 (a) phase entropy loss, (b) effK usage loss 추가\n","# =========================\n","\n","import os\n","import glob\n","import random\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import matplotlib.pyplot as plt\n","from scipy.ndimage import gaussian_filter1d\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","# ---------------------------------------------------------------------\n","# 1) Strict Seeding\n","# ---------------------------------------------------------------------\n","def set_strict_seed(seed: int):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","# ---------------------------------------------------------------------\n","# 2) Data Loading\n","# ---------------------------------------------------------------------\n","def load_mhealth_dataset(data_dir, target_activities_map, column_names):\n","    full_dataset = {}\n","    file_list = sorted(glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\")))\n","\n","    if not file_list:\n","        print(f\"[Warning] No mHealth logs found in {data_dir}\")\n","        return {}\n","\n","    print(f\"Loading {len(file_list)} subjects from {data_dir}...\")\n","\n","    for file_path in file_list:\n","        file_name = os.path.basename(file_path)\n","        subj_part = file_name.split('.')[0]\n","        try:\n","            subj_id_num = int(''.join(filter(str.isdigit, subj_part)))\n","            subj_key = f\"subject{subj_id_num}\"\n","        except:\n","            subj_key = subj_part\n","\n","        try:\n","            df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n","            df = df.iloc[:, :len(column_names)]\n","            df.columns = column_names\n","\n","            subj_data = {}\n","            for label_code, activity_name in target_activities_map.items():\n","                activity_df = df[df['activity_id'] == label_code].copy()\n","                if not activity_df.empty:\n","                    subj_data[activity_name] = activity_df.drop(columns=['activity_id'])\n","\n","            full_dataset[subj_key] = subj_data\n","        except Exception as e:\n","            print(f\"Error loading {file_name}: {e}\")\n","            pass\n","\n","    return full_dataset\n","\n","\n","def prepare_trial_list(label_config, full_data, target_map, feature_map):\n","    trial_list = []\n","    for subj, act_id, gt_count in label_config:\n","        act_name = target_map.get(act_id)\n","        feats = feature_map.get(act_id)\n","\n","        if subj in full_data and act_name in full_data[subj]:\n","            raw_df = full_data[subj][act_name][feats]\n","            raw_np = raw_df.values.astype(np.float32)\n","\n","            # Z-score 정규화 (표준화) 평균=0, std=1\n","            mean = raw_np.mean(axis=0)\n","            std = raw_np.std(axis=0) + 1e-6\n","            norm_np = (raw_np - mean) / std\n","\n","            trial_list.append({\n","                'data': norm_np,\n","                'count': float(gt_count),\n","                'meta': f\"{subj}_{act_name}\"\n","            })\n","        else:\n","            print(f\"[Skip] Missing data for {subj} - {act_name}\")\n","\n","    return trial_list\n","\n","\n","class TrialDataset(Dataset):\n","    def __init__(self, trial_list):\n","        self.trials = trial_list\n","\n","    def __len__(self):\n","        return len(self.trials)\n","\n","    def __getitem__(self, idx):\n","        item = self.trials[idx]\n","        data = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1)  # (C, T)\n","        count = torch.tensor(item['count'], dtype=torch.float32)\n","        return data, count, item['meta']\n","\n","\n","def collate_variable_length(batch):\n","    max_len = max([x[0].shape[1] for x in batch])\n","    C = batch[0][0].shape[0]\n","\n","    padded_data, masks, counts, metas, lengths = [], [], [], [], []\n","    for data, count, meta in batch:\n","        T = data.shape[1]\n","        lengths.append(T)\n","\n","        pad_size = max_len - T\n","        if pad_size > 0:\n","            pad = torch.zeros(C, pad_size)\n","            d_padded = torch.cat([data, pad], dim=1)\n","            mask = torch.cat([torch.ones(T), torch.zeros(pad_size)], dim=0)\n","        else:\n","            d_padded = data\n","            mask = torch.ones(T)\n","\n","        padded_data.append(d_padded)\n","        masks.append(mask)\n","        counts.append(count)\n","        metas.append(meta)\n","\n","    return {\n","        \"data\": torch.stack(padded_data),         # (B, C, T_max)\n","        \"mask\": torch.stack(masks),               # (B, T_max)\n","        \"count\": torch.stack(counts),             # (B,)\n","        \"length\": torch.tensor(lengths, dtype=torch.float32),  # (B,)\n","        \"meta\": metas\n","    }\n","\n","\n","# ---------------------------------------------------------------------\n","# 3) Model\n","# ---------------------------------------------------------------------\n","class ManifoldEncoder(nn.Module):\n","    def __init__(self, input_ch, hidden_dim=128, latent_dim=16):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv1d(input_ch, hidden_dim, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv1d(hidden_dim, latent_dim, 1)\n","        )\n","\n","    def forward(self, x):\n","        # x: (B, C, T)\n","        z = self.net(x)            # (B, D, T)\n","        z = z.transpose(1, 2)      # (B, T, D)\n","        return z\n","\n","\n","class ManifoldDecoder(nn.Module):\n","    def __init__(self, latent_dim, hidden_dim, out_ch):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv1d(latent_dim, hidden_dim, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv1d(hidden_dim, hidden_dim, 5, padding=2),\n","            nn.ReLU(),\n","            nn.Conv1d(hidden_dim, out_ch, 1)\n","        )\n","\n","    def forward(self, z):\n","        # z: (B, T, D)\n","        zt = z.transpose(1, 2)     # (B, D, T)\n","        x_hat = self.net(zt)       # (B, C, T)\n","        return x_hat\n","\n","\n","class MultiRateHead(nn.Module):\n","    def __init__(self, latent_dim=16, hidden=64, K_max=6):\n","        super().__init__()\n","        self.K_max = K_max\n","        self.net = nn.Sequential(\n","            nn.Linear(latent_dim, hidden),\n","            nn.ReLU(),\n","            nn.Linear(hidden, 1 + K_max)  # [amp_logit | phase_logits...]\n","        )\n","\n","    def forward(self, z, tau=1.0):\n","        # z: (B,T,D)\n","        out = self.net(z)                     # (B,T,1+K)\n","        amp = F.softplus(out[..., 0])         # (B,T) >=0  (total micro intensity)\n","        phase_logits = out[..., 1:]           # (B,T,K)\n","        phase = F.softmax(phase_logits / tau, dim=-1)  # (B,T,K), sum=1\n","        return amp, phase, phase_logits\n","\n","\n","class KAutoCountModel(nn.Module):\n","    \"\"\"\n","    - outputs K_max micro-event rates r_k(t)\n","    - predicts k_hat (>=1) per sample\n","    - rep_rate(t) = sum_k r_k(t) (모든 micro-event 합) / k_hat\n","    \"\"\"\n","    def __init__(self, input_ch, hidden_dim=128, latent_dim=16, K_max=6, k_hidden=64):\n","        super().__init__()\n","        self.encoder = ManifoldEncoder(input_ch, hidden_dim, latent_dim)\n","        self.decoder = ManifoldDecoder(latent_dim, hidden_dim, input_ch)\n","        self.rate_head = MultiRateHead(latent_dim, hidden=hidden_dim, K_max=K_max)\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv1d, nn.Linear)):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","\n","        with torch.no_grad():\n","            b = self.rate_head.net[-1].bias\n","            b.zero_()\n","            b[0].fill_(-2.0)  # amp logit bias만 -2\n","\n","    @staticmethod\n","    def _masked_mean_time(x, mask=None, eps=1e-6):\n","        # x: (B,T) or (B,T,K)\n","        if mask is None:\n","            return x.mean(dim=1)\n","        if x.dim() == 2:\n","            m = mask.to(dtype=x.dtype, device=x.device)            # (B,T)\n","            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)       # (B,)\n","        elif x.dim() == 3:\n","            m = mask.to(dtype=x.dtype, device=x.device).unsqueeze(-1)  # (B,T,1)\n","            return (x * m).sum(dim=1) / (m.sum(dim=1) + eps)           # (B,K)\n","        else:\n","            raise ValueError(f\"Unsupported dim for masked mean: {x.dim()}\")\n","\n","    def forward(self, x, mask=None, tau=1.0):\n","        \"\"\"\n","        x: (B,C,T), mask: (B,T)\n","        return:\n","          avg_rep_rate: (B,)\n","        \"\"\"\n","        z = self.encoder(x)              # (B,T,D)\n","        x_hat = self.decoder(z)          # (B,C,T)\n","\n","        amp_t, phase_p, phase_logits = self.rate_head(z, tau=1.0)\n","        rates_k_t = amp_t.unsqueeze(-1) * phase_p\n","\n","        # micro-event sum\n","        micro_rate_t = amp_t\n","\n","\n","        p_bar = self._masked_mean_time(phase_p, mask)           # (B,K)\n","        k_hat = 1.0 / (p_bar.pow(2).sum(dim=1) + 1e-6)          # (B,) in [1,K]\n","\n","        # rep rate\n","        rep_rate_t = micro_rate_t / (k_hat.unsqueeze(1) + 1e-6)    # (B,T)\n","        if mask is not None:\n","            rep_rate_t = rep_rate_t * mask\n","\n","        # avg rep rate (masked mean)\n","        if mask is None:\n","            avg_rep_rate = rep_rate_t.mean(dim=1)\n","        else:\n","            avg_rep_rate = (rep_rate_t * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-6)\n","\n","        aux = {\n","            \"rates_k_t\": rates_k_t,          # (B,T,K)\n","            \"phase_p\": phase_p,              # (B,T,K)\n","            \"phase_logits\": phase_logits,    # (B,T,K)\n","            \"micro_rate_t\": micro_rate_t,    # (B,T)\n","            \"rep_rate_t\": rep_rate_t,        # (B,T)\n","            \"k_hat\": k_hat,                  # (B,)\n","        }\n","        return avg_rep_rate, z, x_hat, aux\n","\n","\n","# ---------------------------------------------------------------------\n","# 4) Loss utils\n","# ---------------------------------------------------------------------\n","def masked_recon_mse(x_hat, x, mask, eps=1e-6):\n","    mask = mask.to(dtype=x.dtype, device=x.device)\n","    mask_bc = mask.unsqueeze(1)              # (B,1,T)\n","    se = (x_hat - x) ** 2                    # (B,C,T)\n","    se = se * mask_bc\n","    denom = (mask.sum() * x.shape[1]) + eps  # valid(B*T)*C\n","    return se.sum() / denom\n","\n","\n","def temporal_smoothness(v, mask=None, eps=1e-6):\n","    \"\"\"\n","    v: (B,T) -> L1 smoothness on first difference\n","    \"\"\"\n","    dv = torch.abs(v[:, 1:] - v[:, :-1])  # (B,T-1)\n","    if mask is None:\n","        return dv.mean()\n","    m = mask[:, 1:] * mask[:, :-1]\n","    m = m.to(dtype=dv.dtype, device=dv.device)\n","    return (dv * m).sum() / (m.sum() + eps)\n","\n","\n","def phase_entropy_loss(phase_p, mask=None, eps=1e-8):\n","    \"\"\"\n","    time-wise exclusivity: 각 t에서 phase가 one-hot에 가까워지게(entropy 최소화)\n","    phase_p: (B,T,K)\n","    \"\"\"\n","    ent = -(phase_p * (phase_p + eps).log()).sum(dim=-1)  # (B,T)\n","    if mask is None:\n","        return ent.mean()\n","    ent = ent * mask\n","    return ent.sum() / (mask.sum() + eps)\n","\n","\n","def effK_usage_loss(phase_p, mask=None, eps=1e-6):\n","    \"\"\"\n","    overall usage sparsity: time-avg phase usage의 effective-K를 줄이게\n","    effK = 1 / sum(p_bar^2)  in [1,K]\n","    \"\"\"\n","    if mask is None:\n","        p_bar = phase_p.mean(dim=1)  # (B,K)\n","    else:\n","        m = mask.to(dtype=phase_p.dtype, device=phase_p.device).unsqueeze(-1)  # (B,T,1)\n","        p_bar = (phase_p * m).sum(dim=1) / (m.sum(dim=1) + eps)\n","\n","    effK = 1.0 / (p_bar.pow(2).sum(dim=1) + eps)\n","    return effK.mean(), effK.detach()\n","\n","\n","# ---------------------------------------------------------------------\n","# 5) Train\n","# ---------------------------------------------------------------------\n","def train_one_epoch(model, loader, optimizer, config, device):\n","    model.train()\n","    stats = {k: 0.0 for k in [\n","        'loss', 'loss_rate', 'loss_recon', 'loss_smooth', 'loss_phase_ent', 'loss_effk',\n","        'mae_count'\n","    ]}\n","\n","    fs = config[\"fs\"]\n","    tau = config.get(\"tau\", 1.0)\n","\n","    lam_recon = config.get(\"lambda_recon\", 1.0)\n","    lam_smooth = config.get(\"lambda_smooth\", 0.05)\n","    lam_phase_ent = config.get(\"lambda_phase_ent\", 0.01)\n","    lam_effk = config.get(\"lambda_effk\", 0.005)\n","\n","    for batch in loader:\n","        x = batch[\"data\"].to(device)         # (B,C,T)\n","        mask = batch[\"mask\"].to(device)      # (B,T)\n","        y_count = batch[\"count\"].to(device)  # (B,)\n","        length = batch[\"length\"].to(device)  # (B,)\n","\n","        duration = torch.clamp(length / fs, min=1e-6)  # sec\n","        y_rate = y_count / duration                    # reps/s\n","\n","        optimizer.zero_grad()\n","\n","        rate_hat, z, x_hat, aux = model(x, mask, tau=tau)\n","\n","        # (1) 속도 맞추기 (MSE Loss)\n","        loss_rate = F.mse_loss(rate_hat, y_rate)\n","\n","        # (2) recon\n","        loss_recon = masked_recon_mse(x_hat, x, mask)\n","\n","        # (3) smoothness (rep_rate_t 기준)\n","        loss_smooth = temporal_smoothness(aux[\"rep_rate_t\"], mask)\n","\n","        # (4) phase exclusivity (entropy)\n","        loss_phase_ent = phase_entropy_loss(aux[\"phase_p\"], mask)\n","\n","        # (5) effective-K usage (overall)\n","        loss_effk, _ = effK_usage_loss(aux[\"phase_p\"], mask)\n","\n","        loss = (loss_rate\n","                + lam_recon * loss_recon\n","                + lam_smooth * loss_smooth\n","                + lam_phase_ent * loss_phase_ent\n","                + lam_effk * loss_effk)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # MAE on count\n","        count_hat = rate_hat * duration  # 예측 속도 * 시간 = 예측 개수\n","        stats['loss'] += loss.item()\n","        stats['loss_rate'] += loss_rate.item()\n","        stats['loss_recon'] += loss_recon.item()\n","        stats['loss_smooth'] += loss_smooth.item()\n","        stats['loss_phase_ent'] += loss_phase_ent.item()\n","        stats['loss_effk'] += loss_effk.item()\n","        stats['mae_count'] += torch.abs(count_hat - y_count).mean().item()\n","\n","    n = len(loader)\n","    return {k: v / n for k, v in stats.items()}\n","\n","\n","# ---------------------------------------------------------------------\n","# 6) Visualization helpers (subject-wise subplot)\n","# ---------------------------------------------------------------------\n","def _smooth_1d(y, sigma=2.0):\n","    \"\"\"\n","    지저분한 노이즈를 다듬어서 시각화\n","    \"\"\"\n","    y = np.asarray(y, dtype=np.float32)\n","    return gaussian_filter1d(y, sigma=sigma)\n","\n","\n","def compute_phase_entropy_mean(phase_p_np, eps=1e-8):\n","    \"\"\"\n","    phase_p_np: (T,K) numpy\n","    return: time-avg entropy (scalar)\n","    \"\"\"\n","    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n","    ent_t = -(phase_p_np * np.log(phase_p_np + eps)).sum(axis=1)  # (T,)\n","    return float(ent_t.mean())\n","\n","def downsample_time_axis(arr, max_T=2000):\n","    \"\"\"\n","    arr: (T, ...) numpy\n","    너무 길면 시각화가 깨지므로 T를 max_T로 downsample\n","    return: arr_ds, idx (원래 시간 인덱스)\n","    \"\"\"\n","    T = arr.shape[0]\n","    if T <= max_T:\n","        idx = np.arange(T)\n","        return arr, idx\n","    idx = np.linspace(0, T - 1, max_T).astype(int)\n","    return arr[idx], idx\n","\n","\n","def plot_phase_heatmap_and_dominant(\n","    phase_p_np,\n","    fs,\n","    title=\"phase_p heatmap + dominant phase\",\n","    max_T=2000\n","):\n","    \"\"\"\n","    phase_p_np: (T, K) numpy array\n","    fs: sampling rate\n","    max_T: 시각화 다운샘플링 길이\n","    \"\"\"\n","    phase_p_np = np.asarray(phase_p_np, dtype=np.float32)\n","    assert phase_p_np.ndim == 2, f\"phase_p_np must be (T,K), got {phase_p_np.shape}\"\n","\n","    # (1) downsample for visualization\n","    phase_ds, idx = downsample_time_axis(phase_p_np, max_T=max_T)  # (T',K)\n","    Tds, K = phase_ds.shape\n","    t_sec = idx / float(fs)\n","\n","    # (2) dominant phase (argmax)\n","    dom = np.argmax(phase_ds, axis=1)  # (T',)\n","\n","    # (3) plot\n","    fig = plt.figure(figsize=(30, 10))\n","    gs = fig.add_gridspec(2, 1, height_ratios=[4, 1], hspace=0.25)\n","\n","    # --- Heatmap (top) ---\n","    ax0 = fig.add_subplot(gs[0, 0])\n","    # imshow expects (rows, cols) -> we want y=K, x=T\n","    im = ax0.imshow(\n","        phase_ds.T,                 # (K, T')\n","        aspect=\"auto\",\n","        origin=\"lower\",\n","        interpolation=\"nearest\",\n","        extent=[t_sec[0], t_sec[-1], 0, K]   # x=time, y=phase index range\n","    )\n","    ax0.set_title(title, fontsize=24, pad=10)\n","    ax0.set_ylabel(\"Phase k\", fontsize=18)\n","    ax0.set_xlabel(\"Time (sec)\", fontsize=18)\n","    cbar = fig.colorbar(im, ax=ax0, fraction=0.015, pad=0.01)\n","    cbar.set_label(\"phase_p(t,k)\", fontsize=14)\n","\n","    # --- Dominant phase timeline (bottom) ---\n","    ax1 = fig.add_subplot(gs[1, 0], sharex=ax0)\n","    # dominant phase를 1행짜리 이미지로 보여주면 가장 깔끔함\n","    ax1.imshow(\n","        dom[None, :],               # (1, T')\n","        aspect=\"auto\",\n","        origin=\"lower\",\n","        interpolation=\"nearest\",\n","        extent=[t_sec[0], t_sec[-1], 0, 1]\n","    )\n","    ax1.set_yticks([])\n","    ax1.set_ylabel(\"dominant\", fontsize=14)\n","    ax1.set_xlabel(\"Time (sec)\", fontsize=18)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def plot_folds_test_subplot(viz_cache, fs, title=\"Fold-wise TEST visualization (only test_subj)\"):\n","    \"\"\"\n","    viz_cache: list of dict\n","      each dict contains:\n","        - 'fold', 'test_subj', 't', 'rep_rate', 'gt', 'pred', 'diff', 'k_hat', 'entropy'\n","    \"\"\"\n","    if viz_cache is None or len(viz_cache) == 0:\n","        print(\"[plot_folds_test_subplot] viz_cache is empty\")\n","        return\n","\n","    sns.set_theme(style=\"whitegrid\", context=\"notebook\", font_scale=2.0)\n","    colors = sns.color_palette(\"muted\")\n","    c_rate = colors[0]\n","    c_count = colors[1]\n","\n","    n = len(viz_cache)\n","    fig, axes = plt.subplots(n, 1, figsize=(36, 9 * n), sharex=False)\n","    if n == 1:\n","        axes = [axes]\n","    axes = np.array(axes).flatten()\n","\n","    fig.suptitle(title, fontsize=40, y=0.995)\n","\n","    for i, item in enumerate(viz_cache):\n","        ax = axes[i]\n","\n","        t = item[\"t\"]\n","        rep_rate = item[\"rep_rate\"]\n","        gt_count = item[\"gt\"]\n","        pred_count = item[\"pred\"]\n","        diff = item[\"diff\"]\n","        k_hat = item[\"k_hat\"]\n","        entropy = item[\"entropy\"]\n","        test_subj = item[\"test_subj\"]\n","        fold = item[\"fold\"]\n","\n","        rep_s = _smooth_1d(rep_rate, sigma=2.0)\n","        cum = np.cumsum(rep_rate) / fs\n","\n","        # 왼쪽: rep_rate\n","        ax.plot(t, rep_s, color=c_rate, linewidth=2.5, alpha=0.9)\n","        ax.fill_between(t, rep_s, color=c_rate, alpha=0.15)\n","        ax.set_ylabel(\"Rep Rate (reps/s)\", color=c_rate, fontweight='bold', fontsize=24)\n","        ax.grid(True, linestyle='--', alpha=0.5)\n","        ax.tick_params(axis='both', which='major', labelsize=20)\n","\n","        # 오른쪽: cumulative count\n","        ax2 = ax.twinx()\n","        ax2.plot(t, cum, color=c_count, linewidth=3.5, alpha=1.0)\n","        ax2.axhline(gt_count, linestyle=\":\", alpha=0.7)\n","        ax2.set_ylabel(\"Count\", color=c_count, fontweight='bold', fontsize=24)\n","        ax2.tick_params(axis='y', labelcolor=c_count, labelsize=20)\n","        ax2.grid(False)\n","\n","        ax.set_title(\n","            f\"Fold {fold:2d} | Test: {test_subj} | Pred {pred_count:.2f} / GT {gt_count:.0f} (Diff {diff:+.2f})\\n\"\n","            f\"k_hat={k_hat:.2f} | phase_entropy={entropy:.3f}\",\n","            fontsize=34, pad=10\n","        )\n","        ax.set_xlabel(\"Time (sec)\", fontweight='bold', fontsize=24)\n","\n","    plt.tight_layout(rect=[0, 0, 1, 0.985])\n","    plt.subplots_adjust(hspace=0.5)\n","    plt.show()\n","\n","\n","# ---------------------------------------------------------------------\n","# 7) Main (LOSO)\n","# ---------------------------------------------------------------------\n","def main():\n","    CONFIG = {\n","        \"seed\": 42,\n","        \"data_dir\": \"/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\",\n","\n","        \"COLUMN_NAMES\": [\n","            'acc_chest_x', 'acc_chest_y', 'acc_chest_z',\n","            'ecg_1', 'ecg_2',\n","            'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z',\n","            'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z',\n","            'mag_ankle_x', 'mag_ankle_y', 'mag_ankle_z',\n","            'acc_arm_x', 'acc_arm_y', 'acc_arm_z',\n","            'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z',\n","            'mag_arm_x', 'mag_arm_y', 'mag_arm_z',\n","            'activity_id'\n","        ],\n","        \"TARGET_ACTIVITIES_MAP\": {\n","            6: 'Waist bends forward',\n","            7: 'Frontal elevation of arms',\n","            8: 'Knees bending',\n","            12: 'Jump front & back'\n","        },\n","        \"ACT_FEATURE_MAP\": {\n","            6: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z', 'acc_arm_x', 'acc_arm_y', 'acc_arm_z'],\n","            7: ['acc_arm_x', 'acc_arm_y', 'acc_arm_z', 'gyro_arm_x', 'gyro_arm_y', 'gyro_arm_z'],\n","            8: ['acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z', 'gyro_ankle_x', 'gyro_ankle_y', 'gyro_ankle_z'],\n","            12: ['acc_chest_x', 'acc_chest_y', 'acc_chest_z', 'acc_ankle_x', 'acc_ankle_y', 'acc_ankle_z']\n","        },\n","\n","        # Training Params\n","        \"epochs\": 100,\n","        \"lr\": 5e-4,\n","        \"batch_size\": 64,\n","        \"fs\": 50,\n","\n","        # Model\n","        \"hidden_dim\": 128,\n","        \"latent_dim\": 16,\n","        \"K_max\": 6,\n","\n","        # Loss Weights\n","        \"lambda_recon\": 1.0,\n","        \"lambda_smooth\": 0.05,\n","        \"lambda_phase_ent\": 0.01,\n","        \"lambda_effk\": 0.005,\n","\n","        # temperature (phase 경쟁 강도)\n","        \"tau\": 1.0,\n","\n","        # Count-only labels (rep 기준)\n","        # ※ mHealth의 \"Jump front & back\"이 rep=20이라면 20으로 두는 게 맞음.\n","        \"ALL_LABELS\": [\n","            (\"subject1\", 12, 20),\n","            (\"subject2\", 12, 22),\n","            (\"subject3\", 12, 21),\n","            (\"subject4\", 12, 21),\n","            (\"subject5\", 12, 20),\n","            (\"subject6\", 12, 21),\n","            (\"subject7\", 12, 19),\n","            (\"subject8\", 12, 20),\n","            (\"subject9\", 12, 20),\n","            (\"subject10\", 12, 20),\n","        ],\n","    }\n","\n","    set_strict_seed(CONFIG[\"seed\"])\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Device: {device}\")\n","\n","    full_data = load_mhealth_dataset(CONFIG[\"data_dir\"], CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"COLUMN_NAMES\"])\n","    if not full_data:\n","        return\n","\n","    subjects = [f\"subject{i}\" for i in range(1, 11)]\n","    loso_results = []\n","\n","    print(\"\\n\" + \"-\"*80)\n","    print(\" >>> Starting LOSO (count-only, K-auto)\")\n","    print(\"-\"*80)\n","\n","    last_trained_model = None\n","    viz_cache = []\n","\n","    for fold_idx, test_subj in enumerate(subjects):\n","        set_strict_seed(CONFIG[\"seed\"])\n","\n","        train_labels = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] != test_subj]\n","        test_labels  = [x for x in CONFIG[\"ALL_LABELS\"] if x[0] == test_subj]\n","\n","        train_data = prepare_trial_list(train_labels, full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n","        test_data  = prepare_trial_list(test_labels,  full_data, CONFIG[\"TARGET_ACTIVITIES_MAP\"], CONFIG[\"ACT_FEATURE_MAP\"])\n","\n","        if not test_data:\n","            print(f\"[Skip] Fold {fold_idx+1}: {test_subj} has no data.\")\n","            continue\n","\n","        g = torch.Generator()\n","        g.manual_seed(CONFIG[\"seed\"])\n","\n","        train_loader = DataLoader(\n","            TrialDataset(train_data),\n","            batch_size=CONFIG[\"batch_size\"],\n","            shuffle=True,\n","            collate_fn=collate_variable_length,\n","            generator=g,\n","            num_workers=0\n","        )\n","\n","        input_ch = train_data[0]['data'].shape[1]\n","        model = KAutoCountModel(\n","            input_ch=input_ch,\n","            hidden_dim=CONFIG[\"hidden_dim\"],\n","            latent_dim=CONFIG[\"latent_dim\"],\n","            K_max=CONFIG[\"K_max\"]\n","        ).to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n","\n","        for epoch in range(CONFIG[\"epochs\"]):\n","            stats = train_one_epoch(model, train_loader, optimizer, CONFIG, device)\n","            scheduler.step()\n","\n","        model.eval()\n","        last_trained_model = model\n","\n","        # fold test\n","        fold_mae = 0.0\n","        fold_res_str = \"\"\n","\n","        # summary용\n","        test_gt = None\n","        test_pred = None\n","        test_diff = None\n","        test_khat = None\n","        test_entropy = None\n","\n","        # 시각화 캐시용\n","        test_t = None\n","        test_rep_rate = None\n","        test_phase_p = None\n","\n","        for item in test_data:\n","            with torch.no_grad():\n","                x_tensor = torch.tensor(item['data'], dtype=torch.float32).transpose(0, 1).unsqueeze(0).to(device)\n","                duration = x_tensor.shape[2] / CONFIG[\"fs\"]\n","\n","                rate_hat, _, _, aux = model(x_tensor, mask=None)\n","                count_pred = rate_hat.item() * duration # 모델이 예측한 속도(rate) * 시간(duration) = 예측 개수\n","                count_gt = float(item['count'])\n","\n","                abs_err = abs(count_pred - count_gt)  # 실제 개수와의 차이 (절대오차)\n","                fold_mae += abs_err\n","                fold_res_str += f\"[Pred: {count_pred:.1f} / GT: {count_gt:.0f}]\"\n","\n","                # test summary(표현학습 확인용) 값 저장\n","                phase_p = aux[\"phase_p\"].squeeze(0).detach().cpu().numpy()  # (T,K)\n","                k_hat = float(aux[\"k_hat\"].item())\n","                ent = compute_phase_entropy_mean(phase_p)\n","\n","                test_gt = count_gt\n","                test_pred = float(count_pred)\n","                test_diff = float(count_pred - count_gt)\n","                test_khat = k_hat\n","                test_entropy = ent\n","\n","                # subplot용 rep_rate 저장\n","                rep_rate = aux[\"rep_rate_t\"].squeeze(0).detach().cpu().numpy()  # (T,)\n","                T = rep_rate.shape[0]\n","                test_rep_rate = rep_rate\n","                test_t = np.arange(T) / CONFIG[\"fs\"]\n","                test_phase_p = phase_p\n","\n","        fold_mae /= len(test_data)\n","        loso_results.append(fold_mae)\n","\n","        print(f\"Fold {fold_idx+1:2d} | Test: {test_subj} | MAE: {fold_mae:.2f} | {fold_res_str}\")\n","\n","        if (test_gt is not None) and (test_pred is not None):\n","            print(\n","                f\"[Fold TEST Summary] {test_subj} | GT={test_gt:.0f} | Pred={test_pred:.2f} | \"\n","                f\"Diff={test_diff:+.2f} | k_hat={test_khat:.2f} | phase_entropy={test_entropy:.3f}\"\n","            )\n","\n","        if (test_t is not None) and (test_rep_rate is not None):\n","            viz_cache.append({\n","                \"fold\": fold_idx + 1,\n","                \"test_subj\": test_subj,\n","                \"t\": test_t,\n","                \"rep_rate\": test_rep_rate,\n","                \"gt\": float(test_gt) if test_gt is not None else 0.0,\n","                \"pred\": float(test_pred) if test_pred is not None else 0.0,\n","                \"diff\": float(test_diff) if test_diff is not None else 0.0,\n","                \"k_hat\": float(test_khat) if test_khat is not None else 0.0,\n","                \"entropy\": float(test_entropy) if test_entropy is not None else 0.0,\n","                \"phase_p\": phase_p,\n","            })\n","\n","    print(\"-\"*80)\n","    print(f\" >>> Final LOSO Result (Average MAE): {np.mean(loso_results):.3f}\")\n","    print(f\" >>> Standard Deviation: {np.std(loso_results):.3f}\")\n","    print(\"-\"*80)\n","\n","    plot_folds_test_subplot(\n","        viz_cache,\n","        fs=CONFIG[\"fs\"],\n","        title=\"Fold-wise TEST visualization (only test_subj)\"\n","    )\n","\n","    for item in viz_cache:\n","        plot_phase_heatmap_and_dominant(\n","            item[\"phase_p\"],\n","            fs=CONFIG[\"fs\"],\n","            title=f\"[Fold {item['fold']:2d}] {item['test_subj']} | k_hat={item['k_hat']:.2f} | ent={item['entropy']:.3f}\",\n","            max_T=2000\n","        )\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}